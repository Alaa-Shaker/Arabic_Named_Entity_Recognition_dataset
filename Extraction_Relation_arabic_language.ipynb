{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alaa-Shaker/Arabic_Named_Entity_Recognition_dataset/blob/main/Extraction_Relation_arabic_language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0nEF_OOoXpO"
      },
      "source": [
        "\n",
        "# Import data and libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Copyright 2020 Google LLC.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "!pip install tensorflow-gpu --upgrade --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HwZ1yoFldQnC",
        "outputId": "e99638c6-16cb-488c-db5c-0e3d5b447cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow_gpu-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 6.4 kB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.0\n",
            "  Downloading wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 28.4 MB/s \n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting absl-py>=1.0.0\n",
            "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 22.3 MB/s \n",
            "\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 40.4 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.20\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 223 kB/s \n",
            "\u001b[?25hCollecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting typing-extensions>=3.6.6\n",
            "  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
            "Collecting astunparse>=1.6.0\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 43.1 MB/s \n",
            "\u001b[?25hCollecting h5py>=2.9.0\n",
            "  Downloading h5py-3.7.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 30.7 MB/s \n",
            "\u001b[?25hCollecting termcolor>=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 35.3 MB/s \n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting libclang>=13.0.0\n",
            "  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 32.4 MB/s \n",
            "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-62.3.2-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 41.5 MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.1\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting six>=1.12.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 35.3 MB/s \n",
            "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
            "  Downloading grpcio-1.46.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 30.8 MB/s \n",
            "\u001b[?25hCollecting packaging\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting wheel<1.0,>=0.23.0\n",
            "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
            "Collecting requests<3,>=2.21.0\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting werkzeug>=1.0.1\n",
            "  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
            "\u001b[K     |████████████████████████████████| 224 kB 38.3 MB/s \n",
            "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 50.6 MB/s \n",
            "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 31.8 MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[K     |████████████████████████████████| 781 kB 43.1 MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.1.0-py3-none-any.whl (9.2 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 48.2 MB/s \n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting importlib-metadata>=4.4\n",
            "  Downloading importlib_metadata-4.11.4-py3-none-any.whl (18 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 45.1 MB/s \n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
            "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2022.5.18.1-py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 44.8 MB/s \n",
            "\u001b[?25hCollecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 44.6 MB/s \n",
            "\u001b[?25hCollecting pyparsing!=3.0.5,>=2.0.2\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 6.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: termcolor\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=9e9ef77272b1bf2ab8547f46f10af91035e3e3ae75d6641476e712cf31f5939d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
            "Successfully built termcolor\n",
            "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, typing-extensions, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, setuptools, pyparsing, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, packaging, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow-gpu\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyasn1\n",
            "    Found existing installation: pyasn1 0.4.8\n",
            "    Uninstalling pyasn1-0.4.8:\n",
            "      Successfully uninstalled pyasn1-0.4.8\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.0.12\n",
            "    Uninstalling charset-normalizer-2.0.12:\n",
            "      Successfully uninstalled charset-normalizer-2.0.12\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.5.18.1\n",
            "    Uninstalling certifi-2022.5.18.1:\n",
            "      Successfully uninstalled certifi-2022.5.18.1\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.8.0\n",
            "    Uninstalling zipp-3.8.0:\n",
            "      Successfully uninstalled zipp-3.8.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.8\n",
            "    Uninstalling rsa-4.8:\n",
            "      Successfully uninstalled rsa-4.8\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyasn1-modules\n",
            "    Found existing installation: pyasn1-modules 0.2.8\n",
            "    Uninstalling pyasn1-modules-0.2.8:\n",
            "      Successfully uninstalled pyasn1-modules-0.2.8\n",
            "  Attempting uninstall: oauthlib\n",
            "    Found existing installation: oauthlib 3.2.0\n",
            "    Uninstalling oauthlib-3.2.0:\n",
            "      Successfully uninstalled oauthlib-3.2.0\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 4.2.4\n",
            "    Uninstalling cachetools-4.2.4:\n",
            "      Successfully uninstalled cachetools-4.2.4\n",
            "  Attempting uninstall: requests-oauthlib\n",
            "    Found existing installation: requests-oauthlib 1.3.1\n",
            "    Uninstalling requests-oauthlib-1.3.1:\n",
            "      Successfully uninstalled requests-oauthlib-1.3.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.3\n",
            "    Uninstalling importlib-metadata-4.11.3:\n",
            "      Successfully uninstalled importlib-metadata-4.11.3\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 1.35.0\n",
            "    Uninstalling google-auth-1.35.0:\n",
            "      Successfully uninstalled google-auth-1.35.0\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.37.1\n",
            "    Uninstalling wheel-0.37.1:\n",
            "      Successfully uninstalled wheel-0.37.1\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: tensorboard-plugin-wit\n",
            "    Found existing installation: tensorboard-plugin-wit 1.8.1\n",
            "    Uninstalling tensorboard-plugin-wit-1.8.1:\n",
            "      Successfully uninstalled tensorboard-plugin-wit-1.8.1\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.3.7\n",
            "    Uninstalling Markdown-3.3.7:\n",
            "      Successfully uninstalled Markdown-3.3.7\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.46.1\n",
            "    Uninstalling grpcio-1.46.1:\n",
            "      Successfully uninstalled grpcio-1.46.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.0.0\n",
            "    Uninstalling absl-py-1.0.0:\n",
            "      Successfully uninstalled absl-py-1.0.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 1.1.0\n",
            "    Uninstalling termcolor-1.1.0:\n",
            "      Successfully uninstalled termcolor-1.1.0\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.26.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.26.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.26.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "  Attempting uninstall: opt-einsum\n",
            "    Found existing installation: opt-einsum 3.3.0\n",
            "    Uninstalling opt-einsum-3.3.0:\n",
            "      Successfully uninstalled opt-einsum-3.3.0\n",
            "  Attempting uninstall: libclang\n",
            "    Found existing installation: libclang 14.0.1\n",
            "    Uninstalling libclang-14.0.1:\n",
            "      Successfully uninstalled libclang-14.0.1\n",
            "  Attempting uninstall: keras-preprocessing\n",
            "    Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: google-pasta\n",
            "    Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: astunparse\n",
            "    Found existing installation: astunparse 1.6.3\n",
            "    Uninstalling astunparse-1.6.3:\n",
            "      Successfully uninstalled astunparse-1.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires keras<2.9,>=2.8.0rc0, but you have keras 2.9.0 which is incompatible.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tensorboard<2.9,>=2.8, but you have tensorboard 2.9.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "google-api-core 1.31.5 requires google-auth<2.0dev,>=1.25.0, but you have google-auth 2.6.6 which is incompatible.\n",
            "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.1.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.1.0 certifi-2022.5.18.1 charset-normalizer-2.0.12 flatbuffers-1.12 gast-0.4.0 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.46.3 h5py-3.7.0 idna-3.3 importlib-metadata-4.11.4 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 numpy-1.21.6 oauthlib-3.2.0 opt-einsum-3.3.0 packaging-21.3 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-3.0.9 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 setuptools-62.3.2 six-1.16.0 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-estimator-2.9.0 tensorflow-gpu-2.9.1 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0 typing-extensions-4.2.0 urllib3-1.26.9 werkzeug-2.1.2 wheel-0.37.1 wrapt-1.14.1 zipp-3.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy",
                  "pkg_resources",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Import Trax\n",
        "\n",
        "!pip install -q -U trax\n",
        "import trax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF6vi1T-d_70",
        "outputId": "1d7a8601-c983-4702-c14b-35a1cfc59560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 637 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 35.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 6.1 kB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YuyTKAk3CXZ",
        "outputId": "9a7a9c7d-d0a4-4036-824a-6f0d601d33e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting qalsadi\n",
            "  Downloading qalsadi-0.4.5-py3-none-any.whl (256 kB)\n",
            "\u001b[K     |████████████████████████████████| 256 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting Arabic-Stopwords>=0.3\n",
            "  Downloading Arabic_Stopwords-0.3-py3-none-any.whl (353 kB)\n",
            "\u001b[K     |████████████████████████████████| 353 kB 58.6 MB/s \n",
            "\u001b[?25hCollecting pickledb>=0.9.2\n",
            "  Downloading pickleDB-0.9.2.tar.gz (3.7 kB)\n",
            "Collecting naftawayh>=0.3\n",
            "  Downloading Naftawayh-0.4-py3-none-any.whl (332 kB)\n",
            "\u001b[K     |████████████████████████████████| 332 kB 59.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from qalsadi) (0.16.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from qalsadi) (1.15.0)\n",
            "Collecting libqutrub>=1.2.3\n",
            "  Downloading libqutrub-1.2.4.1-py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 56.5 MB/s \n",
            "\u001b[?25hCollecting tashaphyne>=0.3.4.1\n",
            "  Downloading Tashaphyne-0.3.6-py3-none-any.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 46.9 MB/s \n",
            "\u001b[?25hCollecting pyarabic>=0.6.7\n",
            "  Downloading PyArabic-0.6.14-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 57.7 MB/s \n",
            "\u001b[?25hCollecting arramooz-pysqlite>=0.3\n",
            "  Downloading arramooz_pysqlite-0.3-py3-none-any.whl (9.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.2 MB 33.1 MB/s \n",
            "\u001b[?25hCollecting alyahmor>=0.1\n",
            "  Downloading alyahmor-0.1.5-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 421 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pickledb\n",
            "  Building wheel for pickledb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickledb: filename=pickleDB-0.9.2-py3-none-any.whl size=4271 sha256=e0eadee2780103555a96067574ed00836e5e6e3a84ac5280eea33b646dcdf0af\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/34/42/9a7f94099208ce3d32638d98586a5a50f821db2fc75a3bdaae\n",
            "Successfully built pickledb\n",
            "Installing collected packages: pyarabic, tashaphyne, libqutrub, arramooz-pysqlite, pickledb, naftawayh, Arabic-Stopwords, alyahmor, qalsadi\n",
            "Successfully installed Arabic-Stopwords-0.3 alyahmor-0.1.5 arramooz-pysqlite-0.3 libqutrub-1.2.4.1 naftawayh-0.4 pickledb-0.9.2 pyarabic-0.6.14 qalsadi-0.4.5 tashaphyne-0.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: naftawayh in /usr/local/lib/python3.7/dist-packages (0.4)\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.7/dist-packages (from naftawayh) (0.6.14)\n",
            "Requirement already satisfied: tashaphyne in /usr/local/lib/python3.7/dist-packages (from naftawayh) (0.3.6)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pyarabic->naftawayh) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install qalsadi\n",
        "!pip install naftawayh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unrar the dataset for training the model\n",
        "!unrar x \"dataset_Arabic_NER.rar\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Krh83_PXeypU",
        "outputId": "d85f8446-7a12-4b7f-b688-4439104b5046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from dataset_Arabic_NER.rar\n",
            "\n",
            "Creating    dataset_Arabic_NER                                        OK\n",
            "Extracting  dataset_Arabic_NER/info.xlsx                                 \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/tag_map.xlsx                              \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/test                                   OK\n",
            "Creating    dataset_Arabic_NER/test/cooking                           OK\n",
            "Extracting  dataset_Arabic_NER/test/cooking/141.txt                      \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/test/cooking/141.xlsx                     \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/test/geography                         OK\n",
            "Extracting  dataset_Arabic_NER/test/geography/138.txt                    \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/test/geography/138.xlsx                   \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/test/history                           OK\n",
            "Extracting  dataset_Arabic_NER/test/history/143.txt                      \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/test/history/143.xlsx                     \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/test/medical                           OK\n",
            "Extracting  dataset_Arabic_NER/test/medical/140.txt                      \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/test/medical/140.xlsx                     \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/test/news                              OK\n",
            "Extracting  dataset_Arabic_NER/test/news/136.txt                         \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/test/news/136.xlsx                        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/test/news/137.txt                         \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/test/news/137.xlsx                        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/test/sport                             OK\n",
            "Extracting  dataset_Arabic_NER/test/sport/139.txt                        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/test/sport/139.xlsx                       \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/test/technical                         OK\n",
            "Extracting  dataset_Arabic_NER/test/technical/142.txt                    \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/test/technical/142.xlsx                   \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/training                               OK\n",
            "Creating    dataset_Arabic_NER/training/cooking                       OK\n",
            "Extracting  dataset_Arabic_NER/training/cooking/76.txt                   \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/76.xlsx                  \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/77.txt                   \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/77.xlsx                  \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/78.txt                   \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/78.xlsx                  \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/79.txt                   \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/79.xlsx                  \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/80.txt                   \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/80.xlsx                  \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/81.txt                   \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/81.xlsx                  \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/82.txt                   \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/82.xlsx                  \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/83.txt                   \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/83.xlsx                  \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/84.txt                   \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/84.xlsx                  \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/85.txt                   \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/85.xlsx                  \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/86.txt                   \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/86.xlsx                  \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/87.txt                   \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/87.xlsx                  \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/88.txt                   \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/88.xlsx                  \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/89.txt                   \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/89.xlsx                  \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/90.txt                   \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/cooking/90.xlsx                  \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/training/geography                     OK\n",
            "Extracting  dataset_Arabic_NER/training/geography/46.txt                 \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/46.xlsx                \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/47.txt                 \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/47.xlsx                \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/48.txt                 \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/48.xlsx                \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/49.txt                 \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/49.xlsx                \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/50.txt                 \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/50.xlsx                \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/51.txt                 \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/51.xlsx                \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/52.txt                 \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/52.xlsx                \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/53.txt                 \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/53.xlsx                \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/54.txt                 \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/54.xlsx                \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/55.txt                 \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/55.xlsx                \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/56.txt                 \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/56.xlsx                \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/57.txt                 \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/57.xlsx                \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/58.txt                 \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/58.xlsx                \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/59.txt                 \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/59.xlsx                \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/60.txt                 \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/geography/60.xlsx                \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/training/history                       OK\n",
            "Extracting  dataset_Arabic_NER/training/history/31.txt                   \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/31.xlsx                  \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/32.txt                   \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/32.xlsx                  \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/33.txt                   \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/33.xlsx                  \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/34.txt                   \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/34.xlsx                  \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/35.txt                   \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/35.xlsx                  \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/36.txt                   \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/36.xlsx                  \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/37.txt                   \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/37.xlsx                  \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/38.txt                   \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/38.xlsx                  \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/39.txt                   \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/39.xlsx                  \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/40.txt                   \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/40.xlsx                  \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/41.txt                   \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/41.xlsx                  \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/42.txt                   \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/42.xlsx                  \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/43.txt                   \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/43.xlsx                  \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/44.txt                   \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/44.xlsx                  \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/45.txt                   \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/history/45.xlsx                  \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/training/medical                       OK\n",
            "Extracting  dataset_Arabic_NER/training/medical/100.txt                  \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/100.xlsx                 \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/101.txt                  \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/101.xlsx                 \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/102.txt                  \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/102.xlsx                 \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/103.txt                  \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/103.xlsx                 \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/104.txt                  \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/104.xlsx                 \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/105.txt                  \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/105.xlsx                 \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/91.txt                   \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/91.xlsx                  \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/92.txt                   \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/92.xlsx                  \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/93.txt                   \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/93.xlsx                  \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/94.txt                   \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/94.xlsx                  \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/95.txt                   \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/95.xlsx                  \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/96.txt                   \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/96.xlsx                  \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/97.txt                   \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/97.xlsx                  \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/98.txt                   \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/98.xlsx                  \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/99.txt                   \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/medical/99.xlsx                  \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/training/news                          OK\n",
            "Extracting  dataset_Arabic_NER/training/news/1.txt                       \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/1.xlsx                      \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/10.txt                      \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/10.xlsx                     \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/106.txt                     \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/106.xlsx                    \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/107.txt                     \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/107.xlsx                    \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/108.txt                     \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/108.xlsx                    \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/109.txt                     \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/109.xlsx                    \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/11.txt                      \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/11.xlsx                     \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/110.txt                     \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/110.xlsx                    \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/111.txt                     \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/111.xlsx                    \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/112.txt                     \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/112.xlsx                    \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/113.txt                     \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/113.xlsx                    \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/114.txt                     \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/114.xlsx                    \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/115.txt                     \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/115.xlsx                    \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/116.txt                     \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/116.xlsx                    \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/117.txt                     \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/117.xlsx                    \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/118.txt                     \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/118.xlsx                    \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/119.txt                     \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/119.xlsx                    \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/12.txt                      \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/12.xlsx                     \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/120.txt                     \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/120.xlsx                    \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/13.txt                      \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/13.xlsx                     \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/14.txt                      \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/14.xlsx                     \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/15.txt                      \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/15.xlsx                     \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/2.txt                       \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/2.xlsx                      \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/3.txt                       \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/3.xlsx                      \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/4.txt                       \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/4.xlsx                      \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/5.txt                       \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/5.xlsx                      \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/6.txt                       \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/6.xlsx                      \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/7.txt                       \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/7.xlsx                      \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/8.txt                       \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/8.xlsx                      \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/9.txt                       \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/news/9.xlsx                      \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/training/sport                         OK\n",
            "Extracting  dataset_Arabic_NER/training/sport/16.txt                     \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/16.xlsx                    \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/17.txt                     \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/17.xlsx                    \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/18.txt                     \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/18.xlsx                    \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/19.txt                     \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/19.xlsx                    \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/20.txt                     \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/20.xlsx                    \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/21.txt                     \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/21.xlsx                    \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/22.txt                     \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/22.xlsx                    \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/23.txt                     \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/23.xlsx                    \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/24.txt                     \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/24.xlsx                    \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/25.txt                     \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/25.xlsx                    \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/26.txt                     \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/26.xlsx                    \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/27.txt                     \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/27.xlsx                    \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/28.txt                     \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/28.xlsx                    \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/29.txt                     \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/29.xlsx                    \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/30.txt                     \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/sport/30.xlsx                    \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/training/technical                     OK\n",
            "Extracting  dataset_Arabic_NER/training/technical/61.txt                 \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/61.xlsx                \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/62.txt                 \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/62.xlsx                \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/63.txt                 \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/63.xlsx                \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/64.txt                 \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/64.xlsx                \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/65.txt                 \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/65.xlsx                \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/66.txt                 \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/66.xlsx                \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/67.txt                 \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/67.xlsx                \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/68.txt                 \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/68.xlsx                \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/69.txt                 \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/69.xlsx                \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/70.txt                 \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/70.xlsx                \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/71.txt                 \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/71.xlsx                \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/72.txt                 \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/72.xlsx                \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/73.txt                 \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/73.xlsx                \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/74.txt                 \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/74.xlsx                \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/75.txt                 \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/training/technical/75.xlsx                \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/validation                             OK\n",
            "Creating    dataset_Arabic_NER/validation/cooking                     OK\n",
            "Extracting  dataset_Arabic_NER/validation/cooking/130.txt                \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/cooking/130.xlsx               \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/cooking/131.txt                \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/cooking/131.xlsx               \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/validation/geography                   OK\n",
            "Extracting  dataset_Arabic_NER/validation/geography/125.txt              \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/geography/125.xlsx             \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/geography/132.txt              \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/geography/132.xlsx             \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/validation/history                     OK\n",
            "Extracting  dataset_Arabic_NER/validation/history/124.txt                \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/history/124.xlsx               \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/history/126.txt                \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/history/126.xlsx               \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/validation/medical                     OK\n",
            "Extracting  dataset_Arabic_NER/validation/medical/127.txt                \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/medical/127.xlsx               \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/medical/133.txt                \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/medical/133.xlsx               \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/validation/news                        OK\n",
            "Extracting  dataset_Arabic_NER/validation/news/121.txt                   \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/news/121.xlsx                  \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/news/122.txt                   \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/news/122.xlsx                  \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/news/123.txt                   \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/news/123.xlsx                  \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/validation/sport                       OK\n",
            "Extracting  dataset_Arabic_NER/validation/sport/128.txt                  \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/sport/128.xlsx                 \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/sport/129.txt                  \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/sport/129.xlsx                 \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    dataset_Arabic_NER/validation/technical                   OK\n",
            "Extracting  dataset_Arabic_NER/validation/technical/134.txt              \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/technical/134.xlsx             \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/technical/135.txt              \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "Extracting  dataset_Arabic_NER/validation/technical/135.xlsx             \b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dksEBc13GiA"
      },
      "outputs": [],
      "source": [
        "import trax\n",
        "from trax import layers as tl\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "import random as rnd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "sCYNGqQsfoIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDMy3Qai3_Nj"
      },
      "source": [
        "# Read Dataseet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv-g8gbJ3kI4"
      },
      "outputs": [],
      "source": [
        "# 1-step read file (Excel form)\n",
        "\n",
        "def read_dataset(path=\"dataset_Arabic_NER/training/\",choice=\"SENTENCE\"):\n",
        "    sentences=[]\n",
        "    tags=[]\n",
        "    files=[]\n",
        "    classes=[]\n",
        "    classes_type={}\n",
        "    \n",
        "    counter=0\n",
        "    folders = os.listdir(path) #get the main folder\n",
        "    folders\n",
        "    for folder in folders :\n",
        "        classes_type[folder]=counter\n",
        "        counter+=1\n",
        "        \n",
        "    total_number_of_word=0\n",
        "    number_of_files=0\n",
        "    print(\"----- The Dataset contains texts in the following domain -----\")\n",
        "    for folder in folders:\n",
        "        print(folder)\n",
        "        for file in glob.glob(path+folder+\"/*.xlsx\"):\n",
        "            number_of_files+=1\n",
        "            df = pd.read_excel(file)\n",
        "            sentence_number=1\n",
        "            sentence=[]\n",
        "            tag=[]\n",
        "            if choice==\"SENTENCE\":\n",
        "                for i in range (1,(len(df['sentence'].values))):\n",
        "                    if df['sentence'].values[i]==sentence_number:\n",
        "                        sentence.append(df['word'].values[i])\n",
        "                        tag.append(df['tag'].values[i])\n",
        "                    else:\n",
        "                        sentences.append(sentence)\n",
        "                        tags.append(tag)                     \n",
        "                        classes.append(classes_type[folder])\n",
        "                        total_number_of_word+=len(tag)\n",
        "\n",
        "                        if len(tag)==0 or len(tag)==1 : # if there is any error \n",
        "                            print('Error on file ',file)\n",
        "                            print('the tag list :',len(tag))\n",
        "                            print(tag)\n",
        "                            print('the sentence list :',len(sentence))\n",
        "                            print('the sentence number :',sentence_number)                    \n",
        "                            print('the sentence number',df['sentence'].values[i])\n",
        "                        sentence_number+=1\n",
        "                        sentence=[]\n",
        "                        tag=[]                   \n",
        "                        sentence.append(df['word'].values[i])\n",
        "                        tag.append(df['tag'].values[i])\n",
        "                sentences.append(sentence)\n",
        "                tags.append(tag)                     \n",
        "                classes.append(classes_type[folder])\n",
        "                total_number_of_word+=len(tag)\n",
        "\n",
        "            elif choice == \"FILES\": #to read as file\n",
        "                files.append(df['word'].values)\n",
        "                tags.append(df['tag'].values)\n",
        "                classes.append(classes_type[folder])\n",
        "\n",
        "    if choice==\"SENTENCE\" :\n",
        "        print(\"The information about Dataset\")\n",
        "        print(f'The number of files is :{number_of_files}')\n",
        "        print(f'The number of sentences :{len(sentences)}')\n",
        "        print(f'The Total number of word :{total_number_of_word}')\n",
        "        print(\"\\n\\n\")\n",
        "        return sentences,tags,classes\n",
        "\n",
        "    elif choice == \"FILES\": \n",
        "        print(\"The information about Dataset\")\n",
        "        print(f'The number of files is :{len(files)}')\n",
        "        print(\"\\n\\n\")\n",
        "        return files,tags,classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwYP8mSm4DSP",
        "outputId": "f4ddc568-0b43-4b38-d507-987bf4f49ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- The Dataset contains texts in the following domain -----\n",
            "technical\n",
            "geography\n",
            "sport\n",
            "medical\n",
            "history\n",
            "cooking\n",
            "news\n",
            "The information about Dataset\n",
            "The number of files is :120\n",
            "The number of sentences :1731\n",
            "The Total number of word :31326\n",
            "\n",
            "\n",
            "\n",
            "----- The Dataset contains texts in the following domain -----\n",
            "technical\n",
            "geography\n",
            "sport\n",
            "medical\n",
            "history\n",
            "cooking\n",
            "news\n",
            "The information about Dataset\n",
            "The number of files is :15\n",
            "The number of sentences :218\n",
            "The Total number of word :3077\n",
            "\n",
            "\n",
            "\n",
            "----- The Dataset contains texts in the following domain -----\n",
            "technical\n",
            "geography\n",
            "sport\n",
            "medical\n",
            "history\n",
            "cooking\n",
            "news\n",
            "The information about Dataset\n",
            "The number of files is :8\n",
            "The number of sentences :155\n",
            "The Total number of word :1665\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#read the 1-sentences 2-tag of each word in sentence and 3-the classe of sentence\n",
        "\n",
        "path=\"dataset_Arabic_NER/\"\n",
        "\n",
        "t_sentences,t_tags,t_classes=read_dataset(path+\"training/\",choice=\"SENTENCE\") #training data step\n",
        "v_sentences,v_tags,v_classes=read_dataset(path+\"validation/\",choice=\"SENTENCE\") #validation data step\n",
        "test_sentences,test_tags,test_classes=read_dataset(path+\"test/\",choice=\"SENTENCE\")  #test data step\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLx88dkL4GkC",
        "outputId": "2fd4084d-2ad4-4acf-afa0-d25b7de10f11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the sentence in position 100 is:  ['واعتبرت', 'المفوضة', 'الأوروبية', 'لشؤون', 'المنافسة', 'مارغريته', 'فيستاغر', 'في', 'بيان', 'أن', 'هذا', 'القرار', 'يشكل', 'نصراً ', 'للمستهلكين', 'والبيئة']\n",
            "the tags are : ['O', 'B-ORG', 'E-ORG', 'O', 'O', 'B-PER', 'E-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-PER', 'S-MISC']\n",
            "the class of this sentnece 0\n"
          ]
        }
      ],
      "source": [
        "print(\"the sentence in position 100 is: \",t_sentences[100])\n",
        "print(\"the tags are :\",t_tags[100])\n",
        "print(f'the class of this sentnece {t_classes[100]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr7el10g4VAk"
      },
      "source": [
        "# Preprocessing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3LaC3g14SgH",
        "outputId": "5459644d-23d4-4e56-b6b6-c0a3c51df5b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Befor Normalization العُيونِ==العيون result is False\n",
            "After Normalization العُيونِ==العيون result is True\n"
          ]
        }
      ],
      "source": [
        "# 2- step : preprocessing dat\n",
        "# remove undesirable symbol\n",
        "# normailize word\n",
        "\n",
        "def remove_symbol(text):\n",
        "    text=text.replace('<Text>',' ')\n",
        "    text=text.replace('</Text>',' ')\n",
        "    text=text.replace('==',' ')\n",
        "    text=text.replace('===',' ')\n",
        "    text=text.replace(')',' ')\n",
        "    text=text.replace('(',' ')\n",
        "    text=text.replace('}',' ')\n",
        "    text=text.replace('{',' ')\n",
        "    text=text.replace(']',' ')\n",
        "    text=text.replace('[',' ')\n",
        "    #text=text.replace(',',' ')\n",
        "    text=text.replace('.',' ')\n",
        "    text=text.replace(':',' ')\n",
        "    text=text.replace('/',' ')\n",
        "    text=text.replace('\\n',' ')\n",
        "    text=text.replace('\"',' ')\n",
        "    text=text.replace(\"'\",'')\n",
        "    text=text.replace(\"'\\'\",' ')\n",
        "    #text=text.replace('1',' ')\n",
        "    #text=text.replace('2',' ')\n",
        "    #text=text.replace('3',' ')\n",
        "    #text=text.replace('4',' ')\n",
        "    #text=text.replace('5',' ')\n",
        "    #text=text.replace('6',' ')\n",
        "    #text=text.replace('7',' ')\n",
        "    #text=text.replace('8',' ')\n",
        "    #text=text.replace('9',' ')\n",
        "    #text=text.replace('0',' ')\n",
        "    text=text.replace('-',' ')\n",
        "    text=text.replace('+',' ')\n",
        "    text=text.replace('/',' ')\n",
        "    text=text.replace('=',' ')\n",
        "    text=text.replace('*',' ')\n",
        "    text=text.replace('?',' ')\n",
        "    text=text.replace('!',' ')\n",
        "    text=text.replace('\"\\\"',' ')\n",
        "    return text\n",
        "\n",
        "# functions to preprocess the arabic text, T write below the original source of the following function\n",
        "#source https://alraqmiyyat.github.io/2013/01-02.html\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "#This function removes short vowels and other symbols (harakat) that \n",
        "#interfere with computational manipulations with Arabic texts.\n",
        "def deNoise(text):\n",
        "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    text = re.sub(noise, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "#The function unifies the orthography of alifs, hamzas, and yas/alif maqsuras.\n",
        "#This is just a basic function that might need to be modified and expanded for specific purposes.\n",
        "def normalizeArabic(text):\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    return(text)\n",
        "\n",
        "def all_arabic_preprocess_text(text):\n",
        "    text=normalize('NFKC',text)\n",
        "    text=deNoise(text)\n",
        "    #text=normalizeArabic(text)\n",
        "    return text\n",
        "# for exmaple\n",
        "\n",
        "t1=\"العُيونِ\"\n",
        "t2=\"العيون\"\n",
        "print(f'Befor Normalization {t1}=={t2} result is {t1==t2}')\n",
        "t1_=all_arabic_preprocess_text(t1)\n",
        "t2_=all_arabic_preprocess_text(t2)\n",
        "print(f'After Normalization {t1}=={t2} result is {t1_==t2_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsPP8RdQ4c1F"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "def preprocessing(list_of_sentences):\n",
        "    for j,sentence in enumerate(list_of_sentences):\n",
        "        for i,word in enumerate(sentence):\n",
        "            word=str(word)\n",
        "            word=all_arabic_preprocess_text(word)\n",
        "            sentence[i]=word\n",
        "        list_of_sentences[j]=sentence\n",
        "    return list_of_sentences\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WctWsXQB5LXF"
      },
      "outputs": [],
      "source": [
        "preprocess_t_sentences=preprocessing(t_sentences)\n",
        "preprocess_v_sentences=preprocessing(v_sentences)\n",
        "preprocess_test_sentences=preprocessing(test_sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icp2sO-_5R9x"
      },
      "source": [
        "# Build The Dictionary and Decode Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq8vFRHK5UB-"
      },
      "outputs": [],
      "source": [
        "#3-step build dictionary for input word and dictionary for tag\n",
        "\n",
        "def get_vocab(t_sentences, v_sentences,test_sentences):\n",
        "    vocab = {}\n",
        "    vocab['UNK']=0\n",
        "    counter=1\n",
        "    for sentence in t_sentences:         #to ad word in trainind sentences \n",
        "        for word in sentence:\n",
        "            if word not in vocab:                \n",
        "                vocab[word] = counter \n",
        "                counter+=1\n",
        "    for sentence in v_sentences:         #to ad word in validation sentences \n",
        "        for word in sentence:\n",
        "            if word not in vocab:                \n",
        "                vocab[word] = counter  \n",
        "                counter+=1\n",
        "    for sentence in test_sentences:  #to ad word in test sentences      \n",
        "        for word in sentence:\n",
        "            if word not in vocab:                \n",
        "                vocab[word] = counter \n",
        "                counter+=1\n",
        "                \n",
        "        vocab['<PAD>'] = len(vocab)\n",
        "        \n",
        "    \n",
        "    return vocab\n",
        "\n",
        "def get_tag_map(path):  # to build dictionary for coding the tag\n",
        "    file=path+\"tag_map.xlsx\"\n",
        "    df = pd.read_excel(file)\n",
        "    tag_map = {}\n",
        "    for i,tag in enumerate(df['tag']):        \n",
        "        if tag not in tag_map:                \n",
        "            tag_map[tag] = df['code'][i]     \n",
        "    return tag_map\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzyzTUNZ5V8z"
      },
      "outputs": [],
      "source": [
        "vocab=get_vocab(preprocess_t_sentences,preprocess_v_sentences,preprocess_test_sentences)\n",
        "tag_map=get_tag_map(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRca7l5j5Xl4"
      },
      "outputs": [],
      "source": [
        "# convert word and tags into numbers (decoding step)\n",
        "def decod_sentence_and_tags(vocab, tag_map, sentences, tags):\n",
        "    decoded_sentences = []\n",
        "    decoded_labels = []\n",
        "    for sentence in sentences:\n",
        "        s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence]\n",
        "        decoded_sentences.append(s)\n",
        "\n",
        "    for tag in tags:\n",
        "        l = [tag_map.get(label,0) for label in tag] # we use get to give user the ability remove some tag if he wants\n",
        "        decoded_labels.append(l) \n",
        "    return decoded_sentences, decoded_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDUzL3rD5k4Z"
      },
      "outputs": [],
      "source": [
        "decoded_t_sentences, decoded_t_tags=decod_sentence_and_tags(vocab, tag_map, preprocess_t_sentences, t_tags)\n",
        "decoded_v_sentences, decoded_v_tags=decod_sentence_and_tags(vocab, tag_map, preprocess_v_sentences, v_tags)\n",
        "decoded_test_sentences, decoded_test_tags=decod_sentence_and_tags(vocab, tag_map, preprocess_test_sentences, test_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PnDpmVM5bEE"
      },
      "source": [
        "# Build Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8WBimaB5ZzQ"
      },
      "outputs": [],
      "source": [
        "def data_generator(batch_size, x, y, pad, shuffle=False, verbose=False):\n",
        "    '''\n",
        "      Input: \n",
        "        batch_size - integer describing the batch size\n",
        "        x - list containing sentences where words are represented as integers\n",
        "        y - list containing tags associated with the sentences\n",
        "        shuffle - Shuffle the data order\n",
        "        pad - an integer representing a pad character\n",
        "        verbose - Print information during runtime\n",
        "      Output:\n",
        "        a tuple containing 2 elements:\n",
        "        X - np.ndarray of dim (batch_size, max_len) of padded sentences\n",
        "        Y - np.ndarray of dim (batch_size, max_len) of tags associated with the sentences in X\n",
        "    '''\n",
        "    \n",
        "    # count the number of lines in data_lines\n",
        "    num_lines = len(x)\n",
        "    \n",
        "    # create an array with the indexes of data_lines that can be shuffled\n",
        "    lines_index = [*range(num_lines)]\n",
        "    \n",
        "    # shuffle the indexes if shuffle is set to True\n",
        "    if shuffle:\n",
        "        rnd.shuffle(lines_index)\n",
        "    \n",
        "    index = 0 # tracks current location in x, y\n",
        "    while True:\n",
        "        buffer_x = [0] * batch_size # Temporal array to store the raw x data for this batch\n",
        "        buffer_y = [0] * batch_size # Temporal array to store the raw y data for this batch\n",
        "                \n",
        "\n",
        "        \n",
        "        # Copy into the temporal buffers the sentences in x[index : index + batch_size] \n",
        "        # along with their corresponding labels y[index : index + batch_size]\n",
        "        # Find maximum length of sentences in x[index : index + batch_size] for this batch. \n",
        "        # Reset the index if we reach the end of the data set, and shuffle the indexes if needed.\n",
        "        max_len = 0\n",
        "        for i in range(batch_size):\n",
        "             # if the index is greater than or equal to the number of lines in x\n",
        "            if index >= num_lines:\n",
        "                # then reset the index to 0\n",
        "                index = 0\n",
        "                # re-shuffle the indexes if shuffle is set to True\n",
        "                if shuffle:\n",
        "                    rnd.shuffle(lines_index)\n",
        "            \n",
        "            # The current position is obtained using `lines_index[index]`\n",
        "            # Store the x value at the current position into the buffer_x\n",
        "            buffer_x[i] = x[lines_index[index]]\n",
        "            \n",
        "            # Store the y value at the current position into the buffer_y\n",
        "            buffer_y[i] = y[lines_index[index]]\n",
        "            \n",
        "            lenx = len(x[lines_index[index]])    #length of current x[]\n",
        "            if lenx > max_len:\n",
        "                max_len = lenx                   #max_len tracks longest x[]\n",
        "            \n",
        "            # increment index by one\n",
        "            index += 1\n",
        "\n",
        "\n",
        "        # create X,Y, NumPy arrays of size (batch_size, max_len) 'full' of pad value\n",
        "        X = np.full((batch_size,max_len),pad)\n",
        "        Y = np.full((batch_size,max_len),pad)\n",
        "\n",
        "        # copy values from lists to NumPy arrays. Use the buffered values\n",
        "        for i in range(batch_size):\n",
        "            # get the example (sentence as a tensor)\n",
        "            # in `buffer_x` at the `i` index\n",
        "            x_i = buffer_x[i]\n",
        "            \n",
        "            # similarly, get the example's labels\n",
        "            # in `buffer_y` at the `i` index\n",
        "            y_i = buffer_y[i]\n",
        "            \n",
        "            # Walk through each word in x_i\n",
        "            for j in range(len(x_i)):\n",
        "                # store the word in x_i at position j into X\n",
        "                X[i, j] = x_i[j]\n",
        "                \n",
        "                # store the label in y_i at position j into Y\n",
        "                Y[i, j] = y_i[j]\n",
        "\n",
        "        if verbose: print(\"index=\", index)\n",
        "        yield((X,Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wdIzxE35oDb",
        "outputId": "8e29341a-e520-4985-c955-586f23afc16d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index= 5\n",
            "index= 2\n",
            "(5, 42) (5, 42) (5, 32) (5, 32)\n",
            "[    1     2     3     4     5     6     7     8     9    10    11    12\n",
            "    13    14    15    16    17 11737 11737 11737 11737 11737 11737 11737\n",
            " 11737 11737 11737 11737 11737 11737 11737 11737 11737 11737 11737 11737\n",
            " 11737 11737 11737 11737 11737 11737] \n",
            " [    0     0     0     0     0     0    29     0     0     0     0     0\n",
            "     0     0     0     0     0 11737 11737 11737 11737 11737 11737 11737\n",
            " 11737 11737 11737 11737 11737 11737 11737 11737 11737 11737 11737 11737\n",
            " 11737 11737 11737 11737 11737 11737]\n"
          ]
        }
      ],
      "source": [
        "batch_size = 5\n",
        "mini_sentences = decoded_t_sentences[0: 8]\n",
        "mini_labels = decoded_t_tags[0: 8]\n",
        "dg = data_generator(batch_size, mini_sentences, mini_labels, vocab[\"<PAD>\"], shuffle=False, verbose=True)\n",
        "X1, Y1 = next(dg)\n",
        "X2, Y2 = next(dg)\n",
        "print(Y1.shape, X1.shape, Y2.shape, X2.shape)\n",
        "print(X1[0][:], \"\\n\", Y1[0][:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhjRg2Uu5sgL"
      },
      "source": [
        "# Build model LSTM and train it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e12BF0IJ5xhT"
      },
      "outputs": [],
      "source": [
        "from trax.supervised import training\n",
        "\n",
        "rnd.seed(33)\n",
        "\n",
        "batch_size = 512\n",
        "\n",
        "# Create training data, mask pad id=11737 for training.\n",
        "train_generator = trax.data.inputs.add_loss_weights(\n",
        "    data_generator(batch_size, decoded_t_sentences, decoded_t_tags, vocab['<PAD>'], True),\n",
        "    id_to_mask=vocab['<PAD>'])\n",
        "\n",
        "# Create validation data, mask pad id=11737 for training.\n",
        "eval_generator = trax.data.inputs.add_loss_weights(\n",
        "    data_generator(batch_size, decoded_v_sentences, decoded_v_tags, vocab['<PAD>'], True),\n",
        "    id_to_mask=vocab['<PAD>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU24C_MB50NJ"
      },
      "outputs": [],
      "source": [
        "def train_model(NER, train_generator, eval_generator, train_steps=1, output_dir='model'):\n",
        "    '''\n",
        "    Input: \n",
        "        NER - the model you are building\n",
        "        train_generator - The data generator for training examples\n",
        "        eval_generator - The data generator for validation examples,\n",
        "        train_steps - number of training steps\n",
        "        output_dir - folder to save your model\n",
        "    Output:\n",
        "        training_loop - a trax supervised training Loop\n",
        "    '''\n",
        "\n",
        "    train_task = training.TrainTask(\n",
        "      train_generator, # A train data generator\n",
        "      loss_layer = tl.CrossEntropyLoss(), # A cross-entropy loss function\n",
        "      optimizer = trax.optimizers.Adam(0.01),  # The adam optimizer\n",
        "    )\n",
        "\n",
        "    eval_task = training.EvalTask(\n",
        "      labeled_data = eval_generator, # A labeled data generator\n",
        "      metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], # Evaluate with cross-entropy loss and accuracy\n",
        "      n_eval_batches = 10 # Number of batches to use on each evaluation\n",
        "    )\n",
        "\n",
        "    training_loop = training.Loop(\n",
        "        NER, # A model to train\n",
        "        train_task, # A train task\n",
        "        eval_tasks=[eval_task] , # The evaluation task\n",
        "        output_dir=output_dir ) # The output directory\n",
        "\n",
        "    # Train with train_steps\n",
        "    training_loop.run(n_steps = train_steps)\n",
        "\n",
        "    return training_loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3sfBjcX56au"
      },
      "outputs": [],
      "source": [
        "def NER_LSTM(vocab_size=len(vocab), d_model=100, tags=tag_map):\n",
        "    '''\n",
        "      Input: \n",
        "        vocab_size - integer containing the size of the vocabulary\n",
        "        d_model - integer describing the embedding size\n",
        "      Output:\n",
        "        model - a trax serial model\n",
        "    '''\n",
        "    model = tl.Serial(\n",
        "      tl.Embedding(vocab_size,d_model), # Embedding layer\n",
        "      tl.LSTM(n_units=d_model), # LSTM layer\n",
        "      tl.Dense(n_units=len(tags)), # Dense layer with len(tags) units\n",
        "      tl.LogSoftmax()  # LogSoftmax layer\n",
        "      )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNT5SjO-6F_i",
        "outputId": "579a7fd7-73b5-46d8-a884-de740a253105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LSTM model with 1000 iterations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
            "/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py:446: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 1257837\n",
            "Step      1: Ran 1 train steps in 3.40 secs\n",
            "Step      1: train CrossEntropyLoss |  3.89636636\n",
            "Step      1: eval  CrossEntropyLoss |  2.12702610\n",
            "Step      1: eval          Accuracy |  0.65333531\n",
            "\n",
            "Step    100: Ran 99 train steps in 72.10 secs\n",
            "Step    100: train CrossEntropyLoss |  0.80253345\n",
            "Step    100: eval  CrossEntropyLoss |  1.03036102\n",
            "Step    100: eval          Accuracy |  0.82933178\n",
            "\n",
            "Step    200: Ran 100 train steps in 56.66 secs\n",
            "Step    200: train CrossEntropyLoss |  0.04749086\n",
            "Step    200: eval  CrossEntropyLoss |  1.28690464\n",
            "Step    200: eval          Accuracy |  0.82298955\n",
            "\n",
            "Step    300: Ran 100 train steps in 54.75 secs\n",
            "Step    300: train CrossEntropyLoss |  0.00971773\n",
            "Step    300: eval  CrossEntropyLoss |  1.40153309\n",
            "Step    300: eval          Accuracy |  0.81606081\n",
            "\n",
            "Step    400: Ran 100 train steps in 55.21 secs\n",
            "Step    400: train CrossEntropyLoss |  0.00501174\n",
            "Step    400: eval  CrossEntropyLoss |  1.47380234\n",
            "Step    400: eval          Accuracy |  0.81099229\n",
            "\n",
            "Step    500: Ran 100 train steps in 59.06 secs\n",
            "Step    500: train CrossEntropyLoss |  0.00356152\n",
            "Step    500: eval  CrossEntropyLoss |  1.53860406\n",
            "Step    500: eval          Accuracy |  0.80665163\n",
            "\n",
            "Step    600: Ran 100 train steps in 55.34 secs\n",
            "Step    600: train CrossEntropyLoss |  0.00288600\n",
            "Step    600: eval  CrossEntropyLoss |  1.58738464\n",
            "Step    600: eval          Accuracy |  0.81013285\n",
            "\n",
            "Step    700: Ran 100 train steps in 57.00 secs\n",
            "Step    700: train CrossEntropyLoss |  0.00250531\n",
            "Step    700: eval  CrossEntropyLoss |  1.62929857\n",
            "Step    700: eval          Accuracy |  0.80769143\n",
            "\n",
            "Step    800: Ran 100 train steps in 54.44 secs\n",
            "Step    800: train CrossEntropyLoss |  0.00228775\n",
            "Step    800: eval  CrossEntropyLoss |  1.65944822\n",
            "Step    800: eval          Accuracy |  0.80796250\n",
            "\n",
            "Step    900: Ran 100 train steps in 58.18 secs\n",
            "Step    900: train CrossEntropyLoss |  0.00211933\n",
            "Step    900: eval  CrossEntropyLoss |  1.68394626\n",
            "Step    900: eval          Accuracy |  0.80663696\n",
            "\n",
            "Step   1000: Ran 100 train steps in 54.95 secs\n",
            "Step   1000: train CrossEntropyLoss |  0.00201464\n",
            "Step   1000: eval  CrossEntropyLoss |  1.69589647\n",
            "Step   1000: eval          Accuracy |  0.80538661\n",
            "The time need to train the LSTM model is 597.9867799282074 for 1000 iterations\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "train_steps = 1000    \n",
        "!rm -f 'model_LSTM_5000/model.pkl.gz'  # Remove old model.pkl if it exists\n",
        "print(f'The LSTM model with {train_steps} iterations')\n",
        "start_time = time.time()\n",
        "# Train the model\n",
        "training_loop = train_model(NER_LSTM(), train_generator, eval_generator, train_steps,'model_LSTM_5000')\n",
        "\n",
        "print(f'The time need to train the LSTM model is {(time.time() - start_time)} for {train_steps} iterations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9650Vg96WHB"
      },
      "source": [
        "# Load the LSTM model and measure the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGrQcINJ2oCp"
      },
      "outputs": [],
      "source": [
        "#save the model to local storage\n",
        "#!apt-get install rar\n",
        "\n",
        "#!rar a \"/content/model_LSTM_5000_saved\" \"/content/model_LSTM_5000\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NewE_Kb6IS_",
        "outputId": "741fb811-080a-4273-804c-494d548106ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([[ 0.11866585,  0.13302152, -0.12722465, ..., -0.01180588,\n",
              "           0.04755793,  0.10067556],\n",
              "         [-0.1411955 , -0.12251778, -0.00446306, ..., -0.08757155,\n",
              "           0.04917156, -0.02836037],\n",
              "         [-0.07472941, -0.01327013, -0.13650274, ...,  0.07872515,\n",
              "          -0.05980076,  0.06099828],\n",
              "         ...,\n",
              "         [-0.08289175, -0.08703338,  0.09031789, ...,  0.1202599 ,\n",
              "           0.01743716, -0.00301366],\n",
              "         [-0.09266497,  0.1533098 ,  0.1439503 , ..., -0.14762057,\n",
              "           0.09659383,  0.12624641],\n",
              "         [ 0.14286686,  0.13370726, -0.1347514 , ..., -0.1191118 ,\n",
              "          -0.16756457,  0.06243087]], dtype=float32),\n",
              "  (((), ((), ())),\n",
              "   ((array([[-0.152566  , -0.01308115,  0.15794808, ..., -0.57527965,\n",
              "              0.53460366, -0.03982986],\n",
              "            [-0.11746692,  0.02239569, -0.11516903, ...,  0.33989534,\n",
              "             -0.50465554,  0.50919545],\n",
              "            [-0.05561784, -0.07823023, -0.3329275 , ..., -0.3110191 ,\n",
              "             -0.211824  ,  0.02335083],\n",
              "            ...,\n",
              "            [-0.03677461,  0.12814182,  0.17010072, ..., -0.04403535,\n",
              "             -0.02355125,  0.14174409],\n",
              "            [ 0.04600276,  0.0492399 , -0.21809568, ...,  0.18212572,\n",
              "             -0.05370449, -0.01480335],\n",
              "            [-0.01898591,  0.14435421,  0.25594762, ...,  0.11208201,\n",
              "             -0.37527165,  0.09804516]], dtype=float32),\n",
              "     array([1.165239  , 1.1463704 , 1.2956227 , 1.001308  , 1.0135008 ,\n",
              "            1.3531237 , 1.3843293 , 1.1575538 , 0.97252536, 0.9829838 ,\n",
              "            0.99749327, 1.0024971 , 1.1324573 , 1.3290455 , 1.158281  ,\n",
              "            1.3837739 , 1.1347615 , 1.005649  , 1.3348316 , 1.3477151 ,\n",
              "            1.0031368 , 1.1408856 , 1.2558627 , 1.3903301 , 1.2800872 ,\n",
              "            1.104295  , 1.1768361 , 1.1848419 , 1.3228364 , 0.9575229 ,\n",
              "            1.1335467 , 1.1374372 , 1.3961738 , 1.036164  , 1.1661687 ,\n",
              "            0.930189  , 1.5065051 , 1.2461498 , 1.5843835 , 1.2150562 ,\n",
              "            1.191472  , 1.0011163 , 0.9341004 , 1.2610404 , 0.9729511 ,\n",
              "            1.5005918 , 1.1343057 , 1.2879066 , 1.2107397 , 1.0466443 ,\n",
              "            1.1521538 , 1.7004814 , 0.8662784 , 1.3837509 , 1.1231751 ,\n",
              "            1.0116038 , 1.2312696 , 1.2312125 , 1.239988  , 1.1279894 ,\n",
              "            1.3045889 , 1.2868748 , 1.3630494 , 1.2461778 , 1.1730063 ,\n",
              "            1.0296371 , 1.020195  , 0.9055089 , 0.9974122 , 1.3398643 ,\n",
              "            1.2104425 , 1.1332452 , 1.138784  , 1.2967799 , 1.0159211 ,\n",
              "            1.0507152 , 1.1850519 , 0.9403543 , 1.1583167 , 1.0754697 ,\n",
              "            1.4048018 , 1.1257814 , 1.2723742 , 1.1787888 , 1.4083308 ,\n",
              "            1.2669615 , 1.4220189 , 1.3162638 , 0.986844  , 1.2999653 ,\n",
              "            1.2032276 , 1.3140599 , 1.552432  , 1.2036698 , 0.9316003 ,\n",
              "            1.337536  , 1.0834756 , 1.2275124 , 1.1172472 , 1.4412731 ,\n",
              "            1.0503056 , 1.0781322 , 0.76526904, 0.83605486, 0.91937083,\n",
              "            0.94996965, 1.2119255 , 1.1287421 , 0.9115408 , 0.7603956 ,\n",
              "            0.8750599 , 0.8911019 , 1.055969  , 1.189691  , 1.0797284 ,\n",
              "            0.8789826 , 0.9158535 , 0.829506  , 1.0953434 , 1.0886117 ,\n",
              "            0.9221794 , 1.1494839 , 1.2004243 , 0.75906926, 1.1793468 ,\n",
              "            0.83523625, 1.1362126 , 0.90745986, 1.1044189 , 0.7968376 ,\n",
              "            0.82855874, 0.79639816, 1.3267651 , 0.8981261 , 0.99191344,\n",
              "            0.9014507 , 0.9862367 , 1.0965949 , 1.2073522 , 1.115593  ,\n",
              "            1.0968508 , 0.87320656, 0.8781406 , 1.1850375 , 0.9108602 ,\n",
              "            1.2600754 , 1.0801878 , 1.1758641 , 1.1151882 , 0.918203  ,\n",
              "            0.703981  , 0.617996  , 0.9797522 , 1.2050858 , 0.7508135 ,\n",
              "            0.92093825, 1.1077982 , 1.0995324 , 1.0465827 , 0.7051828 ,\n",
              "            1.2579134 , 1.2107658 , 0.8784737 , 1.057595  , 1.0690958 ,\n",
              "            0.8961644 , 0.9080991 , 0.9185868 , 0.9050032 , 1.2119863 ,\n",
              "            1.056169  , 1.0799953 , 1.0857779 , 1.1769298 , 0.89987415,\n",
              "            0.92660564, 1.0855137 , 0.9471786 , 1.0817212 , 0.93401885,\n",
              "            0.85172874, 0.95701945, 1.1072577 , 0.9457308 , 1.2535261 ,\n",
              "            1.0904555 , 1.2287008 , 1.2096381 , 0.891961  , 1.1305939 ,\n",
              "            0.9008274 , 1.1597806 , 1.0866596 , 1.1612797 , 0.92954916,\n",
              "            1.1347238 , 0.9208957 , 1.1017106 , 0.70213217, 0.9792427 ,\n",
              "            1.010938  , 1.0484809 , 0.8164245 , 0.9566957 , 0.9229818 ,\n",
              "            0.9821783 , 1.0379131 , 1.0462042 , 0.93109363, 0.9788945 ,\n",
              "            1.0243014 , 0.9439374 , 1.0591741 , 1.0693562 , 1.0397148 ,\n",
              "            0.90206933, 0.9376004 , 0.8763654 , 1.0755535 , 1.074329  ,\n",
              "            0.93565714, 1.0546657 , 1.0538244 , 0.85193825, 1.047514  ,\n",
              "            0.91560966, 1.0453204 , 1.0415385 , 1.0829296 , 1.0060195 ,\n",
              "            0.89378047, 0.8845122 , 0.8800757 , 0.9104088 , 0.992413  ,\n",
              "            0.90290767, 1.0339084 , 1.0159444 , 1.0794961 , 1.0579283 ,\n",
              "            1.0489659 , 1.029615  , 0.9758347 , 1.0578907 , 0.90200585,\n",
              "            1.0911194 , 1.0547754 , 1.0569941 , 1.0298548 , 0.88577694,\n",
              "            0.9533308 , 0.9942424 , 1.01011   , 1.0660949 , 0.88799393,\n",
              "            0.90221334, 1.0547934 , 1.035615  , 1.0579892 , 0.7685657 ,\n",
              "            1.0539707 , 1.059505  , 1.0217448 , 0.96133745, 1.0462478 ,\n",
              "            0.93194383, 0.94862384, 0.95185584, 0.9358554 , 1.0528566 ,\n",
              "            1.0384539 , 1.0561429 , 1.0477711 , 1.0126019 , 0.8936772 ,\n",
              "            0.94097805, 1.0473318 , 0.9546411 , 1.052555  , 0.9305868 ,\n",
              "            0.9407169 , 0.89366   , 1.061689  , 0.920273  , 1.0736985 ,\n",
              "            1.0592695 , 1.0594051 , 0.8916965 , 0.9513977 , 1.0136328 ,\n",
              "            0.8679812 , 1.0680339 , 1.0931225 , 1.0495613 , 0.92675686,\n",
              "            1.0157688 , 0.90943706, 1.0537103 , 0.9386485 , 0.9380812 ,\n",
              "            1.0627117 , 1.0300783 , 1.2742205 , 0.9978512 , 0.9966032 ,\n",
              "            1.1748989 , 0.97077805, 1.0261699 , 0.97908545, 1.0198163 ,\n",
              "            0.97309303, 0.9796886 , 1.036907  , 1.0532682 , 1.0682853 ,\n",
              "            1.20894   , 1.0329311 , 0.98608315, 1.0512224 , 1.1501473 ,\n",
              "            0.9903914 , 1.0385885 , 1.0293261 , 1.0390143 , 1.0764432 ,\n",
              "            1.0146266 , 1.065698  , 0.831629  , 1.114308  , 0.8527497 ,\n",
              "            1.0209926 , 1.0320249 , 1.1299801 , 1.006731  , 1.081502  ,\n",
              "            0.9464214 , 1.1228997 , 1.077402  , 1.1506677 , 1.0945362 ,\n",
              "            1.0327686 , 0.9565506 , 0.8896306 , 1.0751773 , 0.962181  ,\n",
              "            1.1211759 , 1.0466814 , 1.0390743 , 1.0542842 , 1.0009753 ,\n",
              "            1.0381814 , 1.1721013 , 0.8964302 , 1.073958  , 1.0549148 ,\n",
              "            0.9913787 , 1.1140896 , 1.0692887 , 1.1117702 , 1.1524502 ,\n",
              "            1.1196336 , 1.0928093 , 1.1969954 , 0.91919327, 1.154642  ,\n",
              "            1.022256  , 1.0213318 , 0.897356  , 1.0172316 , 1.0301923 ,\n",
              "            0.99735874, 1.0872273 , 1.0551288 , 1.0497917 , 1.049859  ,\n",
              "            1.0228027 , 1.0659077 , 0.9058208 , 1.0188997 , 1.0580438 ,\n",
              "            1.0685232 , 0.9250868 , 1.0755197 , 0.9365548 , 1.0989361 ,\n",
              "            1.058712  , 0.9514916 , 1.0888344 , 0.9930912 , 1.0842667 ,\n",
              "            1.0569255 , 1.0801611 , 1.1092074 , 1.0498934 , 0.9258255 ,\n",
              "            1.0482752 , 1.0062481 , 1.0170368 , 0.96767926, 1.2348549 ],\n",
              "           dtype=float32)),),\n",
              "   ()),\n",
              "  (array([[ 0.20311855, -0.29709947, -0.17173335, ..., -0.09658056,\n",
              "            0.1016256 ,  0.15399145],\n",
              "          [ 0.20876285,  0.18277127, -0.0433858 , ..., -0.12382028,\n",
              "           -0.24738877,  0.04414082],\n",
              "          [ 0.13528122,  0.97086835, -0.4261609 , ..., -0.4704911 ,\n",
              "           -0.76313597,  0.7810806 ],\n",
              "          ...,\n",
              "          [ 0.20883134, -0.23421098, -0.33751422, ...,  0.42476794,\n",
              "           -0.00613191, -0.24709275],\n",
              "          [ 0.13292968, -0.4043971 ,  0.05916133, ...,  0.44140497,\n",
              "           -0.1288754 ,  0.4387871 ],\n",
              "          [ 0.06594416,  0.44501883, -0.6339711 , ..., -0.17498805,\n",
              "           -0.55981463, -0.2935377 ]], dtype=float32),\n",
              "   array([ 0.10843486, -0.08528823, -0.05455865, -0.27045614, -0.2213141 ,\n",
              "           0.00818685, -0.05223237, -0.15034108, -0.26329538,  0.01424618,\n",
              "           0.04707718, -0.06992621, -0.04056192,  0.06151589,  0.08554059,\n",
              "          -0.06763453, -0.10469361, -0.06218019, -0.00863452, -0.24469586,\n",
              "          -0.31618735,  0.01060114, -0.19206336, -0.21820681, -0.27949777,\n",
              "          -0.0093439 ,  0.04457655, -0.11231665, -0.11819108, -0.0326011 ,\n",
              "          -0.01161568, -0.09873207, -0.19713494,  0.03057957,  0.04845686,\n",
              "          -0.1388934 , -0.27892455], dtype=float32)),\n",
              "  ()),\n",
              " ((), (((), ((), ())), ((), ()), ()), (), ()))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# loading in a pretrained model..\n",
        "model = NER_LSTM()\n",
        "model.init(trax.shapes.ShapeDtype((1, 1), dtype=np.int32))\n",
        "\n",
        "# Load the pretrained model\n",
        "model.init_from_file('model_LSTM_5000/model.pkl.gz', weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5GUTL2Q8nUC"
      },
      "outputs": [],
      "source": [
        "def evaluate_prediction(pred, labels, pad):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        pred: prediction array with shape \n",
        "            (num examples, max sentence length in batch, num of classes)\n",
        "        labels: array of size (batch_size, seq_len)\n",
        "        pad: integer representing pad character\n",
        "    Outputs:\n",
        "        accuracy: float\n",
        "    \"\"\"\n",
        "\n",
        "    outputs = np.argmax(pred,axis=-1)\n",
        "    print(\"outputs shape:\", outputs.shape)\n",
        "\n",
        "    mask = ~(labels==pad)\n",
        "    print(\"mask shape:\", mask.shape, \"mask[0][20:30]:\", mask[0][20:30])\n",
        "\n",
        "    accuracy = np.sum(outputs==labels)/np.sum(mask)\n",
        " \n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N830Adr8wM0",
        "outputId": "38530277-ff04-44a7-809c-3564d99b0208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shapes (155, 24) (155, 24)\n"
          ]
        }
      ],
      "source": [
        "# the test data\n",
        "x, y = next(data_generator(len(decoded_test_sentences), decoded_test_sentences, decoded_test_tags, vocab['<PAD>']))\n",
        "print(\"input shapes\", x.shape, y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpGgTzMn7vWF",
        "outputId": "749573f9-4d52-4a71-9bfb-dcbec07e3142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "outputs shape: (155, 24)\n",
            "mask shape: (155, 24) mask[0][20:30]: [False False False False]\n",
            "The accuracy of LSTM model with 1000 iterations is : 0.7987987995147705 \n"
          ]
        }
      ],
      "source": [
        "accuracy = evaluate_prediction(model(x), y, vocab['<PAD>'])\n",
        "print(f'The accuracy of LSTM model with {train_steps} iterations is : {accuracy} ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peqHs9BA80eY"
      },
      "source": [
        "# Test the LSTM model by predicting the NER of test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjxXlPQZ7POm"
      },
      "outputs": [],
      "source": [
        "# This is the function you will be using to test your own sentence.\n",
        "def predict(sentence, model, vocab, tag_map):\n",
        "    s = [vocab[all_arabic_preprocess_text(token)] if token in vocab else vocab['UNK'] for token in sentence.split(' ')] ############\n",
        "    batch_data = np.ones((1, len(s)))\n",
        "    batch_data[0][:] = s\n",
        "    sentence = np.array(batch_data).astype(int)\n",
        "    output = model(sentence)\n",
        "    outputs = np.argmax(output, axis=2)\n",
        "    labels = list(tag_map.keys())\n",
        "    pred = []\n",
        "    for i in range(len(outputs[0])):\n",
        "        idx = outputs[0][i] \n",
        "        pred_label = labels[idx]\n",
        "        pred.append(pred_label)\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzYMvNt47Rlz",
        "outputId": "e319c538-4cf1-4275-8889-30651c4b0cf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "الكاميرات S-MISC\n",
            "بريطانيا S-GPE\n",
            "قادة S-PRO\n"
          ]
        }
      ],
      "source": [
        "# New york times news:\n",
        "sentence = \"الكاميرات ترصد ملكة بريطانيا وهي تشتكي قادة العالم \"\n",
        "#s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
        "predictions = predict(sentence, model, vocab, tag_map)\n",
        "for x,y in zip(sentence.split(' '), predictions):\n",
        "    if y != 'O':\n",
        "        print(x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxHcN57a9RDi",
        "outputId": "3f9b2883-1c57-4464-e5a2-bc048e7c52b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nimport xlwt\\n\\ndef output(data):\\n  workbook = xlwt.Workbook()\\n  sheet = workbook.add_sheet(\"LSTM_results\")\\n  sheet.write(0, 0, \\'Query Words\\')\\n  sheet.write(0, 1, \\'The word\\')\\n  sheet.write(0, 2, \\'The find NER Tag\\')\\n  row_counter=1\\n  for item,value in results.items():\\n    sheet.write(row_counter,0,str(item))\\n    row_counter+=1\\n    for q_item,q_value in value.items():\\n      sheet.write(row_counter,1,str(q_item))\\n      sheet.write(row_counter,2,str(q_value))\\n      row_counter+=1\\n\\n  workbook.save(\\'LSTM_results_10000.xls\\')\\n\\noutput(results) \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "'''\n",
        "\n",
        "import xlwt\n",
        "\n",
        "def output(data):\n",
        "  workbook = xlwt.Workbook()\n",
        "  sheet = workbook.add_sheet(\"LSTM_results\")\n",
        "  sheet.write(0, 0, 'Query Words')\n",
        "  sheet.write(0, 1, 'The word')\n",
        "  sheet.write(0, 2, 'The find NER Tag')\n",
        "  row_counter=1\n",
        "  for item,value in results.items():\n",
        "    sheet.write(row_counter,0,str(item))\n",
        "    row_counter+=1\n",
        "    for q_item,q_value in value.items():\n",
        "      sheet.write(row_counter,1,str(q_item))\n",
        "      sheet.write(row_counter,2,str(q_value))\n",
        "      row_counter+=1\n",
        "\n",
        "  workbook.save('LSTM_results_10000.xls')\n",
        "\n",
        "output(results) \n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CmqUZdqSKRh"
      },
      "source": [
        "# Function make lemmatizer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVZo4IVgriHz"
      },
      "source": [
        "## 1- Farsa lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSGMV9vKSarU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "import os\n",
        "import codecs\n",
        "url = 'https://farasa.qcri.org/webapi/lemmatization/'\n",
        "text = 'يُشار إلى أن اللغة العربية' \n",
        "api_key = \"geyKnGORqoJznYhMYn\"\n",
        "\n",
        "def arabic_lemmatizer_farasa(text):\n",
        "    payload = {'text': text, 'api_key': api_key}\n",
        "    data = requests.post(url, data=payload)\n",
        "    result = json.loads(data.text)\n",
        "    return result\n",
        "result =arabic_lemmatizer_farasa(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqYQ2y8artKV"
      },
      "source": [
        "## 2- Qalsadi lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEV-eInircPy"
      },
      "outputs": [],
      "source": [
        "import qalsadi.lemmatizer\n",
        "def arabic_lemmatizer_qalsadi(text):\n",
        "  lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
        "  return lemmer.lemmatize_text(text)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(arabic_lemmatizer_qalsadi('وأشار'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bcEMWLB2ZnM",
        "outputId": "d5d0f2df-25eb-4fdb-c5bc-e843f865ec83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['شار']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3- classifying function"
      ],
      "metadata": {
        "id": "5gyZ7tT1nL0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import naftawayh.wordtag\n",
        "\n",
        "def classify_arabic_word(word):\n",
        "  tagger = naftawayh.wordtag.WordTagger()\n",
        "  if tagger.is_verb(word):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "'''\n",
        "word_list=\"الكاميرات ترصد ملكة بريطانيا وهي تشتكي على قادة العالم\".split(' ')\n",
        "tagger = naftawayh.wordtag.WordTagger()\n",
        "for word in word_list:\n",
        "  if tagger.is_noun(word):\n",
        "    print(u'%s is noun'%word)\n",
        "  if tagger.is_verb(word):\n",
        "    print(u'%s is verb'%word)\n",
        "  if tagger.is_stopword(word):\n",
        "    print(u'%s is stopword'%word)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "BYYt_i39nWV0",
        "outputId": "cc80d991-77ae-459b-e26b-acb39d1de639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nword_list=\"الكاميرات ترصد ملكة بريطانيا وهي تشتكي على قادة العالم\".split(\\' \\')\\ntagger = naftawayh.wordtag.WordTagger()\\nfor word in word_list:\\n  if tagger.is_noun(word):\\n    print(u\\'%s is noun\\'%word)\\n  if tagger.is_verb(word):\\n    print(u\\'%s is verb\\'%word)\\n  if tagger.is_stopword(word):\\n    print(u\\'%s is stopword\\'%word)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Imot9s5VTd37"
      },
      "source": [
        "# function for reading Excel file contains lemm verbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNcaMfqHSsjf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "b11810b6-9ffe-477a-9ad5-4ccda409c7da"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6e48421c4849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mlemm_verbs_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtranslated_verbs_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/translated_list_verb_noun.xlsx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlemm_verbs_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtranslated_verbs_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-6e48421c4849>\u001b[0m in \u001b[0;36mread_test\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m    \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m    \u001b[0mlemm_verbs_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arabic_verb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m    \u001b[0mtranslated_verbs_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'translated_verb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 ext = inspect_excel_format(\n\u001b[0;32m-> 1192\u001b[0;31m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m                 )\n\u001b[1;32m   1194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m     with get_handle(\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     ) as handle:\n\u001b[1;32m   1073\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/translated_list_verb_noun.xlsx'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def read_test(file_path):\n",
        "   df = pd.read_excel(file_path)\n",
        "   lemm_verbs_list=df['arabic_verb'].values\n",
        "   translated_verbs_list=df['translated_verb'].values\n",
        "\n",
        "   return lemm_verbs_list,translated_verbs_list\n",
        "file_path='/content/translated_list_verb_noun.xlsx'\n",
        "lemm_verbs_list,translated_verbs_list=read_test(file_path) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "s_w6ZVocVQuw",
        "outputId": "8762f86d-631b-4451-d24e-84093f7e456d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f52e2adb7973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemm_verbs_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated_verbs_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lemm_verbs_list' is not defined"
          ]
        }
      ],
      "source": [
        "print(lemm_verbs_list[40])\n",
        "print(translated_verbs_list[40])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBj-DfF6Vw3X"
      },
      "source": [
        "# function find lemm words in list lemm verb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIIG09KnVoi2"
      },
      "outputs": [],
      "source": [
        "def find_lemm_word_in_list(word,lemm_verbs_list):\n",
        "  counter=0\n",
        "  index=-1\n",
        "  for verb in lemm_verbs_list:\n",
        "    if word== verb:\n",
        "      index=counter\n",
        "    counter+=1\n",
        "  return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnB18hubX9B8",
        "outputId": "a323880e-9e6a-4870-9872-bfe0a5d6c613"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ],
      "source": [
        "find_lemm_word_in_list('أبحر',lemm_verbs_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cd9NZX9YE2w"
      },
      "outputs": [],
      "source": [
        "def find_indexs_sentence_in_list_verb(sentence,lem_sentence_list, lemm_verbs_list,classify_arabic_word=classify_arabic_word):\n",
        "  map_index={}\n",
        "  i=0\n",
        "  for word in sentence.split(' '):\n",
        "    if(len(word)>1):\n",
        "      index=find_lemm_word_in_list(lem_sentence_list[i],lemm_verbs_list)\n",
        "      if(index>=0 ):  #and classify_arabic_word(word)\n",
        "        map_index[word]=index\n",
        "      i=i+1\n",
        "      \n",
        "  return map_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIEuAeG_rW9v"
      },
      "source": [
        "# Preprocessing the input sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMd8bH-fucRM",
        "outputId": "a6059e3e-43fe-4c38-abac-92583dd58a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "import nltk \n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0WQJMulrdCn",
        "outputId": "59e55d77-1fcc-4306-cb42-f95237fe2bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "الكاميرات ترصد ملكة بريطانيا وهي تشتكي قادة العالم \n"
          ]
        }
      ],
      "source": [
        "# remove stopwprds \n",
        "\n",
        "def remove_arabic_stopwords(text):\n",
        "  arb_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
        "  if (len(text)>2):\n",
        "    new_text=''\n",
        "    for word in text.split(\" \"):\n",
        "      if word not in arb_stopwords:\n",
        "          new_text=new_text+word+' '\n",
        "    return new_text\n",
        "  else:\n",
        "    print(f'Error the length of input text is less than ',str(len(text)))\n",
        "\n",
        "text=\"الكاميرات ترصد ملكة بريطانيا وهي تشتكي على قادة العالم\"\n",
        "print(remove_arabic_stopwords(text)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucI5FpqE11wU"
      },
      "outputs": [],
      "source": [
        "def prepocess_input_sentence(text):\n",
        "  new_text=all_arabic_preprocess_text(text)\n",
        "  new_text=remove_arabic_stopwords(new_text)\n",
        "  return new_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7xYT5MDo6fI"
      },
      "source": [
        "# The final function that combins all steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZODDHCQkp_n",
        "outputId": "44e4d569-bac4-4791-872a-dd94e4ef29c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['أن', 'ملك', 'الآشوري', 'شلمنصر', 'أول', 'استولى', 'بابل']\n"
          ]
        }
      ],
      "source": [
        "sentence = \"الكاميرات ترصد ملكة بريطانيا وهي تشتكي قادة العالم\"\n",
        "sen=\"أن الملك الآشوري شلمنصر الأول استولى بابل\"\n",
        "\n",
        "result =arabic_lemmatizer_qalsadi(sen) \n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3ZeY_clpfea",
        "outputId": "1cb9d606-2a56-4d23-e931-d273601e157b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ['كامرا', 'رصد', 'ملك', 'بريطانيا', 'هي', 'اشتكى', 'قائد', 'عالم']}"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ],
      "source": [
        "sentence = \"الكاميرات ترصد ملكة بريطانيا وهي تشتكي قادة العالم \"\n",
        "\n",
        "arabic_lemmatizer_farasa(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5491x5PAmp9"
      },
      "source": [
        "## remove name Entity function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvcLeBeuAk3i"
      },
      "outputs": [],
      "source": [
        "def remove_name_entity(sentence,list_name_entities):\n",
        "  new_sentence=''\n",
        "  for word in sentence.split(' '):\n",
        "    if word not in list_name_entities :\n",
        "      new_sentence+=word+' '\n",
        "  return new_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## function group entity's name"
      ],
      "metadata": {
        "id": "JbBMege20b2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def group_entity_name(sentence,label_list):\n",
        "  group_entity_name=''\n",
        "  list_entities=[]\n",
        "  group_lable_entity=''\n",
        "  list_labled=[]\n",
        "  split_sentence=sentence.split(' ')\n",
        "  for x,y in zip(split_sentence, label_list):\n",
        "    if 'B-' in y:\n",
        "      group_entity_name=group_entity_name+x+' '\n",
        "      group_lable_entity=group_lable_entity+y+' '\n",
        "    if 'I-' in y:\n",
        "      group_entity_name=group_entity_name+x+' '\n",
        "      group_lable_entity=group_lable_entity+y+' '\n",
        "    #if 'E-' in y:\n",
        "      #group_entity_name=group_entity_name+x+' '\n",
        "      #group_lable_entity=group_lable_entity+y+' '\n",
        "    if 'S-' in y:\n",
        "      group_entity_name=group_entity_name+x+' '\n",
        "      group_lable_entity=group_lable_entity+y+' '\n",
        "\n",
        "    if 'E-' in y :\n",
        "      group_entity_name=group_entity_name+x+' '\n",
        "      group_lable_entity=group_lable_entity+y+' '\n",
        "      list_entities.append(group_entity_name)\n",
        "      list_labled.append(group_lable_entity)\n",
        "      group_entity_name=''\n",
        "      group_lable_entity=''\n",
        "    elif y=='O':\n",
        "      if(len(group_entity_name)>1):\n",
        "        list_entities.append(group_entity_name)\n",
        "        list_labled.append(group_lable_entity)\n",
        "      group_entity_name=''\n",
        "      group_lable_entity=''\n",
        "    \n",
        "  if(len(group_entity_name)>1):\n",
        "        list_entities.append(group_entity_name)\n",
        "        list_labled.append(group_lable_entity) \n",
        "  return list_entities,list_labled\n"
      ],
      "metadata": {
        "id": "U_o1dPz_0pPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## type of grouped entity's name"
      ],
      "metadata": {
        "id": "BkZr4HQuA7x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def type_of_grouped_entity(entity_type):\n",
        "  type_entity=''\n",
        "\n",
        "  if 'TIM' in entity_type:\n",
        "    type_entity= 'TIM'\n",
        "  if 'GPE' in entity_type:\n",
        "    type_entity= 'GPE'\n",
        "  if 'GEO' in entity_type:\n",
        "    type_entity= 'GEO'\n",
        "  if 'LOC' in entity_type:\n",
        "    type_entity= 'LOC'\n",
        "  if 'ORG' in entity_type:\n",
        "    type_entity= 'ORG'\n",
        "  if 'MISC' in entity_type:\n",
        "    type_entity= 'MISC'\n",
        "  if 'DIS' in entity_type:\n",
        "    type_entity= 'DIS'\n",
        "  if 'PRO' in entity_type:\n",
        "    type_entity= 'PRO'\n",
        "  if 'PER' in entity_type:\n",
        "    type_entity= 'PER'\n",
        "  if len(type_entity)>1 and 'TIM' in entity_type and not ('TIM' in type_entity):\n",
        "    type_entity=type_entity+' '+'TIM'\n",
        "  return type_entity\n"
      ],
      "metadata": {
        "id": "T762HWfsBFlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relation_two_entities(verb,index_verb,first_entity,second_entity,labled_first_entity,labled_second_entity,parts_first_entity,parts_second_entity,lemm_verbs_list,translated_verbs_list):\n",
        "  arabic_relational_sentence= verb+'('+first_entity+','+second_entity+')'\n",
        "  print(arabic_relational_sentence)\n",
        "  relation=lemm_verbs_list[index_verb]\n",
        "  print(relation)\n",
        "  labled_relational_sentence=relation+'('+labled_first_entity+','+labled_second_entity+')'\n",
        "  print(labled_relational_sentence)\n",
        "  translated_labled_relational_sentence=translated_verbs_list[index_verb]+'('+labled_first_entity+','+labled_second_entity+')'\n",
        "  print(translated_labled_relational_sentence)\n",
        "  first_entity_labled_parts=parts_first_entity\n",
        "  print(first_entity_labled_parts)\n",
        "  second_entity_labled_parts=parts_second_entity\n",
        "  print(second_entity_labled_parts)\n",
        "  first_entity=first_entity\n",
        "  print(first_entity)\n",
        "  type_First_entity=labled_first_entity\n",
        "  print(type_First_entity)\n",
        "  second_entity=second_entity\n",
        "  print(second_entity)\n",
        "  type_second_entity=labled_second_entity\n",
        "  print(type_second_entity)\n",
        "  return first_entity,second_entity,first_entity_labled_parts,second_entity_labled_parts,type_First_entity,type_second_entity,relation,arabic_relational_sentence,labled_relational_sentence,translated_labled_relational_sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "06hbAzQJT_gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#helper function\n",
        "##--\n",
        "def extract_relation(sentence, map_index,list_entities,list_labled,list_labled_entities,lemm_verbs_list,translated_verbs_list):\n",
        "  #print(map_index, len(map_index))\n",
        "  list_arabic_relational_sentence=[]\n",
        "  list_labled_relational_sentence=[]\n",
        "  list_first_entity_labled_parts=[]\n",
        "  list_second_entity_labled_parts=[]\n",
        "  list_translated_labled_relational_sentence=[]\n",
        "  list_relation=[]\n",
        "  list_first_entity=[]\n",
        "  list_type_First_entity=[]\n",
        "  list_second_entity=[]\n",
        "  list_type_second_entity=[]\n",
        "  list_sentence=[]\n",
        "  list_verb=[]\n",
        "  for key in map_index.keys():\n",
        "    list_verb.append(key)\n",
        "  index_verb=0\n",
        "  if(len(list_entities)==2 or len(list_entities)%2!=0):     \n",
        "      for i in range(1,len(list_entities)):\n",
        "        if(index_verb<len(list_verb)):\n",
        "          verb=list_verb[index_verb]  \n",
        "          index_verb+=1\n",
        "          first_entity,second_entity,first_entity_labled_parts,second_entity_labled_parts,type_First_entity,type_second_entity,relation,arabic_relational_sentence,labled_relational_sentence,translated_labled_relational_sentence=relation_two_entities(verb,map_index[verb],list_entities[0],list_entities[i],list_labled_entities[0],list_labled_entities[i],list_labled[0],list_labled[i],lemm_verbs_list,translated_verbs_list)\n",
        "          list_first_entity.append(first_entity)\n",
        "          list_second_entity.append(second_entity)\n",
        "          list_first_entity_labled_parts.append(first_entity_labled_parts)\n",
        "          list_second_entity_labled_parts.append(second_entity_labled_parts)\n",
        "          list_type_First_entity.append(type_First_entity)\n",
        "          list_type_second_entity.append(type_second_entity)\n",
        "          list_relation.append(relation)\n",
        "          list_arabic_relational_sentence.append(arabic_relational_sentence)\n",
        "          list_labled_relational_sentence.append(labled_relational_sentence)\n",
        "          list_translated_labled_relational_sentence.append(translated_labled_relational_sentence)\n",
        "          list_sentence.append(sentence)\n",
        "  else:\n",
        "    for i in range(0,len(list_entities)): \n",
        "      if(index_verb<len(list_verb)):\n",
        "        verb=list_verb[index_verb]          \n",
        "        index_verb+=1\n",
        "        first_entity,second_entity,first_entity_labled_parts,second_entity_labled_parts,type_First_entity,type_second_entity,relation,arabic_relational_sentence,labled_relational_sentence,translated_labled_relational_sentence=relation_two_entities(verb,map_index[verb],list_entities[i],list_entities[i+1],list_labled_entities[i],list_labled_entities[i+1],list_labled[i],list_labled[i+1],lemm_verbs_list,translated_verbs_list)\n",
        "        list_first_entity.append(first_entity)\n",
        "        list_second_entity.append(second_entity)\n",
        "        list_first_entity_labled_parts.append(first_entity_labled_parts)\n",
        "        list_second_entity_labled_parts.append(second_entity_labled_parts)\n",
        "        list_type_First_entity.append(type_First_entity)\n",
        "        list_type_second_entity.append(type_second_entity)\n",
        "        list_relation.append(relation)\n",
        "        list_arabic_relational_sentence.append(arabic_relational_sentence)\n",
        "        list_labled_relational_sentence.append(labled_relational_sentence)\n",
        "        list_translated_labled_relational_sentence.append(translated_labled_relational_sentence)\n",
        "        list_sentence.append(sentence) \n",
        "  return list_sentence,list_first_entity, list_second_entity ,list_first_entity_labled_parts,list_second_entity_labled_parts,list_type_First_entity,list_type_second_entity,list_relation,list_arabic_relational_sentence,list_labled_relational_sentence,list_translated_labled_relational_sentence\n",
        "    #print(key)\n",
        "    \n"
      ],
      "metadata": {
        "id": "icxr84DA5i9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "# to flat list\n",
        "def flat_list(l):\n",
        "  return list(flatten(l))\n",
        "\n",
        "def flatten(l):\n",
        "    for el in l:\n",
        "        if isinstance(el, collections.abc.Iterable) and not isinstance(el, (str, bytes)):\n",
        "            yield from flatten(el)\n",
        "        else:\n",
        "            yield el"
      ],
      "metadata": {
        "id": "YC4NIbNnQo4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdKphMk6sqrU"
      },
      "outputs": [],
      "source": [
        "sentence = \"الكاميرات ترصد ملكة بريطانيا وهي تشتكي قادة العالم \"\n",
        "def predict_relation_in_sentence(sentence, func_NER_predict=predict,model=model,vocab=vocab,tag_map=tag_map,arabic_lemmatizer=arabic_lemmatizer_qalsadi,func_find_index_words_in_list_verb=find_indexs_sentence_in_list_verb,lemm_verbs_list=lemm_verbs_list,translated_verbs_list=translated_verbs_list,prepocess_input_sentence=prepocess_input_sentence):\n",
        "  #the results holder\n",
        "  predictions = func_NER_predict(sentence, model, vocab, tag_map)\n",
        "  list_entities,list_labled=group_entity_name(sentence,predictions)\n",
        "  #print(list_entities,list_labled)\n",
        "  list_labled_entities=[]\n",
        "  for labled_grouped_entity in list_labled :\n",
        "    list_labled_entities.append(type_of_grouped_entity(labled_grouped_entity))\n",
        "  #print(predictions)\n",
        "  list_name_entities=[]\n",
        "  new_sentence=''  # save all word that has label\n",
        "  for x,y in zip(sentence.split(' '), predictions):\n",
        "      if y != 'O':\n",
        "        new_sentence+=x+' '\n",
        "  if(len(new_sentence)>2):\n",
        "    list_name_entities=prepocess_input_sentence(new_sentence).split(' ') # convert labled words to list\n",
        "  else:\n",
        "    list_name_entities=''\n",
        "  if(len(sentence)>2):\n",
        "    preprocessing_sentence=prepocess_input_sentence(sentence) # to \n",
        "  else:\n",
        "    preprocessing_sentence=''\n",
        "\n",
        "  relation_sentence= remove_name_entity(preprocessing_sentence,list_name_entities) \n",
        "  lemmatized_sentence= arabic_lemmatizer(relation_sentence)\n",
        "  print(sentence)\n",
        "  map_index=func_find_index_words_in_list_verb(relation_sentence,lemmatized_sentence,lemm_verbs_list)\n",
        "  #extract_relation(map_index,list_entities,list_labled,list_labled_entities,lemm_verbs_list)\n",
        "  return extract_relation(sentence,map_index,list_entities,list_labled,list_labled_entities,lemm_verbs_list,translated_verbs_list) \n",
        "  #for x,y in zip(lemmatized_sentence,list_index):\n",
        "    #print(x,y)\n",
        "  \n",
        "  #for x,y in zip(sentence.split(' '),predictions):\n",
        "    #print(x,y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fRPJoiAs36g",
        "outputId": "ebe3989a-2dbe-4bcd-9845-517d9426db57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "الكاميرات ترصد ملكة بريطانيا وهي تشتكي قادة العالم \n",
            "ترصد(الكاميرات ,بريطانيا )\n",
            "ترصد\n",
            "ترصد(MISC,GPE)\n",
            "observe(MISC,GPE)\n",
            "S-MISC \n",
            "S-GPE \n",
            "الكاميرات \n",
            "MISC\n",
            "بريطانيا \n",
            "GPE\n",
            "تشتكي(الكاميرات ,قادة )\n",
            "اشتكى\n",
            "اشتكى(MISC,PRO)\n",
            "complain(MISC,PRO)\n",
            "S-MISC \n",
            "S-PRO \n",
            "الكاميرات \n",
            "MISC\n",
            "قادة \n",
            "PRO\n"
          ]
        }
      ],
      "source": [
        "list_sentence,list_first_entity, list_second_entity ,list_first_entity_labled_parts,list_second_entity_labled_parts,list_type_First_entity,list_type_second_entity,list_relation,list_arabic_relational_sentence,list_labled_relational_sentence,list_translated_labled_relational_sentence=predict_relation_in_sentence(sentence, func_NER_predict=predict,model=model,vocab=vocab,tag_map=tag_map,arabic_lemmatizer=arabic_lemmatizer_qalsadi,func_find_index_words_in_list_verb=find_indexs_sentence_in_list_verb,lemm_verbs_list=lemm_verbs_list,translated_verbs_list=translated_verbs_list,prepocess_input_sentence=prepocess_input_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76IQ5MR2wKxM",
        "outputId": "5a119000-7192-4093-cd26-39ab80d88a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "و اشار غوليايف إلى أن الجيش الروسي باشر كذلك بتوزيع مساعدات إنسانية \n",
            "باشر(غوليايف ,الجيش الروسي )\n",
            "باشر\n",
            "باشر(PER,ORG)\n",
            "start(PER,ORG)\n",
            "S-PER \n",
            "S-ORG S-GPE \n",
            "غوليايف \n",
            "PER\n",
            "الجيش الروسي \n",
            "ORG\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['و اشار غوليايف إلى أن الجيش الروسي باشر كذلك بتوزيع مساعدات إنسانية '],\n",
              " ['غوليايف '],\n",
              " ['الجيش الروسي '],\n",
              " ['S-PER '],\n",
              " ['S-ORG S-GPE '],\n",
              " ['PER'],\n",
              " ['ORG'],\n",
              " ['باشر'],\n",
              " ['باشر(غوليايف ,الجيش الروسي )'],\n",
              " ['باشر(PER,ORG)'],\n",
              " ['start(PER,ORG)'])"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ],
      "source": [
        "sen='و استولى حمورابي ملك بابل على آشور عام 1760 ق.م.'\n",
        "sen_2='ويطلق على الإمبراطورية البابلية في هذهِ المرحلة اسم الإمبراطورية البابلية الثانية'\n",
        "sen_3='و اشار غوليايف إلى أن الجيش الروسي باشر كذلك بتوزيع مساعدات إنسانية '\n",
        "predict_relation_in_sentence(sen_3, func_NER_predict=predict,model=model,vocab=vocab,tag_map=tag_map,arabic_lemmatizer=arabic_lemmatizer_qalsadi,func_find_index_words_in_list_verb=find_indexs_sentence_in_list_verb,lemm_verbs_list=lemm_verbs_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test model for 150 sentences (Excel File)"
      ],
      "metadata": {
        "id": "0oK6sq8pXlY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "path_sentences='/content/arabic_sentence.xlsx'\n",
        "\n",
        "def read_sentence_csv(path_sentences):\n",
        "    df =pd.read_excel(path_sentences)\n",
        "    return df['sentence'],df['type'],df['source_text']\n",
        "\n",
        "sentences,type_sentences,source_sentences= read_sentence_csv(path_sentences)\n",
        "\n",
        "print('the sentence is :', sentences[2])\n",
        "print('the type of sentence is :', type_sentences[2])\n",
        "print('the source of sentence is :',source_sentences[2])"
      ],
      "metadata": {
        "id": "o_eGKmvGK0rM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d92e0f-ac98-402b-c305-936ff8330f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the sentence is :  استولت آشور ثانية على بابل عام 1240 ق.م.\n",
            "the type of sentence is : history\n",
            "the source of sentence is : 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_relation_in_sentence(sentences[5], func_NER_predict=predict,model=model,vocab=vocab,tag_map=tag_map,arabic_lemmatizer=arabic_lemmatizer_qalsadi,func_find_index_words_in_list_verb=find_indexs_sentence_in_list_verb,lemm_verbs_list=lemm_verbs_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2feqkvoYj9L",
        "outputId": "70c4ad45-900d-45f2-e048-75c32c67c3c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "استولى ملك الحيثيين مارسيليس على بابل\n",
            "استولى(ملك الحيثيين مارسيليس ,بابل )\n",
            "استولى\n",
            "استولى(PER,LOC)\n",
            "seized(PER,LOC)\n",
            "S-PRO S-GPE S-PER \n",
            "S-LOC \n",
            "ملك الحيثيين مارسيليس \n",
            "PER\n",
            "بابل \n",
            "LOC\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['استولى ملك الحيثيين مارسيليس على بابل'],\n",
              " ['ملك الحيثيين مارسيليس '],\n",
              " ['بابل '],\n",
              " ['S-PRO S-GPE S-PER '],\n",
              " ['S-LOC '],\n",
              " ['PER'],\n",
              " ['LOC'],\n",
              " ['استولى'],\n",
              " ['استولى(ملك الحيثيين مارسيليس ,بابل )'],\n",
              " ['استولى(PER,LOC)'],\n",
              " ['seized(PER,LOC)'])"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Results"
      ],
      "metadata": {
        "id": "z5tQyftSvvK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_sentences=[]\n",
        "list_first_entity=[]\n",
        "list_second_entity=[]\n",
        "list_first_entity_labled_parts=[]\n",
        "list_second_entity_labled_parts=[]\n",
        "list_type_First_entity=[]\n",
        "list_type_second_entity=[]\n",
        "list_relation=[]\n",
        "list_arabic_relational_sentence=[]\n",
        "list_labled_relational_sentence=[]\n",
        "list_translated_labled_relational_sentence=[]\n",
        "i=1\n",
        "for sentence in sentences:\n",
        "  print('The index of sentence is : ',i)\n",
        "  sentence,first_entity,second_entity,first_entity_labled_parts,second_entity_labled_parts,type_First_entity,type_second_entity,relation,arabic_relational_sentence,labled_relational_sentence,translated_labled_relational_sentence=predict_relation_in_sentence(sentence, func_NER_predict=predict,model=model,vocab=vocab,tag_map=tag_map,arabic_lemmatizer=arabic_lemmatizer_qalsadi,func_find_index_words_in_list_verb=find_indexs_sentence_in_list_verb,lemm_verbs_list=lemm_verbs_list)\n",
        "  list_sentences.append(sentence)\n",
        "  list_first_entity.append(first_entity)\n",
        "  list_second_entity.append(second_entity)\n",
        "  list_first_entity_labled_parts.append(first_entity_labled_parts)\n",
        "  list_second_entity_labled_parts.append(second_entity_labled_parts)\n",
        "  list_type_First_entity.append(type_First_entity)\n",
        "  list_type_second_entity.append(type_second_entity)\n",
        "  list_relation.append(relation)\n",
        "  list_arabic_relational_sentence.append(arabic_relational_sentence)\n",
        "  list_labled_relational_sentence.append(labled_relational_sentence)\n",
        "  list_translated_labled_relational_sentence.append(translated_labled_relational_sentence)\n",
        "  i+=1\n",
        "  print('-----------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh7ft1nHdR34",
        "outputId": "24924351-4d43-4df1-9c94-e9e5a7d8c3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The index of sentence is :  1\n",
            "ولقد حكم الملك شمشي مدينة آشور عام 1813 ق.م.\n",
            "حكم(الملك شمشي مدينة آشور ,عام 1813 ق.م. )\n",
            "حكم\n",
            "حكم(PER,TIM)\n",
            "rule(PER,TIM)\n",
            "S-PRO S-PER B-LOC E-LOC \n",
            "B-TIM I-TIM I-TIM \n",
            "الملك شمشي مدينة آشور \n",
            "PER\n",
            "عام 1813 ق.م. \n",
            "TIM\n",
            "-----------------------------\n",
            "The index of sentence is :  2\n",
            "أن الملك الآشوري شلمنصر الأول استولى على بابل وهزم الميتانيين عام 1273 ق.م\n",
            "استولى(الملك الآشوري شلمنصر الأول ,بابل )\n",
            "استولى\n",
            "استولى(PER,LOC)\n",
            "seized(PER,LOC)\n",
            "S-PRO S-GPE B-PER E-PER \n",
            "B-LOC \n",
            "الملك الآشوري شلمنصر الأول \n",
            "PER\n",
            "بابل \n",
            "LOC\n",
            "وهزم(الملك الآشوري شلمنصر الأول ,الميتانيين عام 1273 ق.م )\n",
            "هزم\n",
            "هزم(PER,GPE TIM)\n",
            "defeat(PER,GPE TIM)\n",
            "S-PRO S-GPE B-PER E-PER \n",
            "S-GPE B-TIM I-TIM E-TIM \n",
            "الملك الآشوري شلمنصر الأول \n",
            "PER\n",
            "الميتانيين عام 1273 ق.م \n",
            "GPE TIM\n",
            "-----------------------------\n",
            "The index of sentence is :  3\n",
            " استولت آشور ثانية على بابل عام 1240 ق.م.\n",
            "استولت(آشور ,بابل عام 1240 ق.م. )\n",
            "استولى\n",
            "استولى(GPE,LOC TIM)\n",
            "seized(GPE,LOC TIM)\n",
            "S-GPE \n",
            "S-LOC B-TIM I-TIM I-TIM \n",
            "آشور \n",
            "GPE\n",
            "بابل عام 1240 ق.م. \n",
            "LOC TIM\n",
            "-----------------------------\n",
            "The index of sentence is :  4\n",
            "وأسر سرجون الثاني اليهود في أورشليم عام 701 ق.م.\n",
            "وأسر(سرجون الثاني ,اليهود )\n",
            "أسر\n",
            "أسر(PER,GPE)\n",
            "capture(PER,GPE)\n",
            "B-PER E-PER \n",
            "S-GPE \n",
            "سرجون الثاني \n",
            "PER\n",
            "اليهود \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  5\n",
            "وهزم حمورابي آشور في عام 1760 ق.م\n",
            "وهزم(حمورابي آشور ,عام 1760 ق.م )\n",
            "هزم\n",
            "هزم(PER,TIM)\n",
            "defeat(PER,TIM)\n",
            "S-PER S-GPE \n",
            "B-TIM I-TIM I-TIM \n",
            "حمورابي آشور \n",
            "PER\n",
            "عام 1760 ق.م \n",
            "TIM\n",
            "-----------------------------\n",
            "The index of sentence is :  6\n",
            "استولى ملك الحيثيين مارسيليس على بابل\n",
            "استولى(ملك الحيثيين مارسيليس ,بابل )\n",
            "استولى\n",
            "استولى(PER,LOC)\n",
            "seized(PER,LOC)\n",
            "S-PRO S-GPE S-PER \n",
            "S-LOC \n",
            "ملك الحيثيين مارسيليس \n",
            "PER\n",
            "بابل \n",
            "LOC\n",
            "-----------------------------\n",
            "The index of sentence is :  7\n",
            "وظهر نبوخدنصر كملك لبابل\n",
            "-----------------------------\n",
            "The index of sentence is :  8\n",
            " إلا أن البابليين قاموا بثورة ضد حكامهم الآشوريين في عام 652 ق.م\n",
            "قاموا(البابليين ,حكامهم الآشوريين )\n",
            "قام\n",
            "قام(GPE,PRO)\n",
            "did(GPE,PRO)\n",
            "S-GPE \n",
            "S-PRO S-GPE \n",
            "البابليين \n",
            "GPE\n",
            "حكامهم الآشوريين \n",
            "PRO\n",
            "-----------------------------\n",
            "The index of sentence is :  9\n",
            "، واستولى نبوخدنصر الثاني على أورشليم عام 587 ق.م\n",
            "-----------------------------\n",
            "The index of sentence is :  10\n",
            "وهزم نبوخدنصر الثاني الفينيقيين عام 585 ق.م\n",
            "وهزم(نبوخدنصر الثاني ,الفينيقيين عام 585 ق.م )\n",
            "هزم\n",
            "هزم(PER,GPE TIM)\n",
            "defeat(PER,GPE TIM)\n",
            "B-PER E-PER \n",
            "S-GPE B-TIM I-TIM E-TIM \n",
            "نبوخدنصر الثاني \n",
            "PER\n",
            "الفينيقيين عام 585 ق.م \n",
            "GPE TIM\n",
            "-----------------------------\n",
            "The index of sentence is :  11\n",
            "وذلك في الفترة التي سيطر خلالها الكلدانيون على بابل\n",
            "سيطر(الكلدانيون ,بابل )\n",
            "سيطر\n",
            "سيطر(GPE,LOC)\n",
            "control(GPE,LOC)\n",
            "S-GPE \n",
            "S-LOC \n",
            "الكلدانيون \n",
            "GPE\n",
            "بابل \n",
            "LOC\n",
            "-----------------------------\n",
            "The index of sentence is :  12\n",
            " ويعتبر نبوخذ نصر أعظم ملوك بابل  605-562 ق.م\n",
            "ويعتبر(نبوخذ نصر ,ملوك بابل )\n",
            "اعتبر\n",
            "اعتبر(PER,PRO)\n",
            "Considered(PER,PRO)\n",
            "B-PER E-PER \n",
            "S-PRO S-GPE \n",
            "نبوخذ نصر \n",
            "PER\n",
            "ملوك بابل \n",
            "PRO\n",
            "-----------------------------\n",
            "The index of sentence is :  13\n",
            "لكلدانيون وعاصمتهم بابل على نهر الفرات\n",
            "-----------------------------\n",
            "The index of sentence is :  14\n",
            "الآشوريون وعاصمتهم نينوى على نهر دجلة\n",
            "-----------------------------\n",
            "The index of sentence is :  15\n",
            "وقد تركزت الحضارة المصرية القديمة 1 على ضفاف نهر النيل\n",
            "-----------------------------\n",
            "The index of sentence is :  16\n",
            "عندما سقطت كليوباترا في أيدي الإمبراطورية الرومانية \n",
            "سقطت(كليوباترا ,الإمبراطورية الرومانية )\n",
            "سقط\n",
            "سقط(PER,GPE)\n",
            "fall(PER,GPE)\n",
            "S-PER \n",
            "B-GPE E-GPE \n",
            "كليوباترا \n",
            "PER\n",
            "الإمبراطورية الرومانية \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  17\n",
            "يعتبر الأمازيغ جزءاً من سكان المغرب\n",
            "يعتبر(الأمازيغ ,سكان المغرب )\n",
            "اعتبر\n",
            "اعتبر(GPE,PER)\n",
            "Considered(GPE,PER)\n",
            "S-GPE \n",
            "S-PER S-GPE \n",
            "الأمازيغ \n",
            "GPE\n",
            "سكان المغرب \n",
            "PER\n",
            "-----------------------------\n",
            "The index of sentence is :  18\n",
            "كانت الإمبراطورية الساسانية تحكم بلاد إيران \n",
            "تحكم(الإمبراطورية الساسانية ,بلاد إيران )\n",
            "تحكم\n",
            "تحكم(GPE,GPE)\n",
            "control(GPE,GPE)\n",
            "B-GPE E-GPE \n",
            "B-GPE E-GPE \n",
            "الإمبراطورية الساسانية \n",
            "GPE\n",
            "بلاد إيران \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  19\n",
            "وكانت عاصمة الدولة في مدينة دمشق\n",
            "-----------------------------\n",
            "The index of sentence is :  20\n",
            " معاوية بن أبي سفيان كان واليًا على الشام \n",
            "واليا(معاوية بن أبي سفيان ,الشام )\n",
            "والي\n",
            "والي(PER,LOC)\n",
            "ruler(PER,LOC)\n",
            "S-PER B-PER I-PER E-PER \n",
            "S-LOC \n",
            "معاوية بن أبي سفيان \n",
            "PER\n",
            "الشام \n",
            "LOC\n",
            "-----------------------------\n",
            "The index of sentence is :  21\n",
            "نشب نزاع بين معاوية بن أبي سفيان و علي بن أبي طالب \n",
            "نشب(معاوية بن أبي سفيان ,علي بن أبي طالب )\n",
            "نشب\n",
            "نشب(PER,PER)\n",
            "begin(PER,PER)\n",
            "B-PER I-PER I-PER E-PER \n",
            "B-PER I-PER I-PER E-PER \n",
            "معاوية بن أبي سفيان \n",
            "PER\n",
            "علي بن أبي طالب \n",
            "PER\n",
            "-----------------------------\n",
            "The index of sentence is :  22\n",
            "أخذ معاوية عن البيزنطيين بعض مظاهر الحكم والإدارة\n",
            "-----------------------------\n",
            "The index of sentence is :  23\n",
            " عمر بن عبد العزيز  يُعد من أفضل الخلفاء الأمويين سيرةً\n",
            "يعد(بن عبد العزيز ,الخلفاء الأمويين )\n",
            "أعاد\n",
            "أعاد(PER,PRO)\n",
            "repeat(PER,PRO)\n",
            "B-PER I-PER E-PER \n",
            "S-PRO S-GPE \n",
            "بن عبد العزيز \n",
            "PER\n",
            "الخلفاء الأمويين \n",
            "PRO\n",
            "-----------------------------\n",
            "The index of sentence is :  24\n",
            "حتى سيطر مروان بن محمد على الخلافة\n",
            "سيطر(مروان بن محمد ,الخلافة )\n",
            "سيطر\n",
            "سيطر(PER,PRO)\n",
            "control(PER,PRO)\n",
            "B-PER I-PER E-PER \n",
            "S-PRO \n",
            "مروان بن محمد \n",
            "PER\n",
            "الخلافة \n",
            "PRO\n",
            "-----------------------------\n",
            "The index of sentence is :  25\n",
            "التقى مروان بن محمد  مع العباسيين في معركة الزاب \n",
            "التقى(مروان بن محمد ,العباسيين )\n",
            "التقى\n",
            "التقى(PER,GPE)\n",
            "meet(PER,GPE)\n",
            "B-PER I-PER E-PER \n",
            "S-GPE \n",
            "مروان بن محمد \n",
            "PER\n",
            "العباسيين \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  26\n",
            "دولة بني العبَّاس هو الاسم الذي يُطلق على ثالث خلافة إسلامية \n",
            "يطلق(دولة بني ,خلافة إسلامية )\n",
            "طلق\n",
            "طلق(PER,GPE)\n",
            "divorced(PER,GPE)\n",
            "B-GPE B-PER \n",
            "S-GPE E-GPE \n",
            "دولة بني \n",
            "PER\n",
            "خلافة إسلامية \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  27\n",
            "استطاع العباسيون أن يزيحوا بني أمية من دربهم\n",
            "استطاع(العباسيون ,بني أمية )\n",
            "استطاع\n",
            "استطاع(GPE,PER)\n",
            "can(GPE,PER)\n",
            "S-GPE \n",
            "B-PER E-PER \n",
            "العباسيون \n",
            "GPE\n",
            "بني أمية \n",
            "PER\n",
            "-----------------------------\n",
            "The index of sentence is :  28\n",
            "وقد اعتمد العباسيون على الفرس الناقمين على الأمويين\n",
            "اعتمد(العباسيون ,الفرس )\n",
            "اعتمد\n",
            "اعتمد(GPE,GPE)\n",
            "depend(GPE,GPE)\n",
            "S-GPE \n",
            "S-GPE \n",
            "العباسيون \n",
            "GPE\n",
            "الفرس \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  29\n",
            "نقل المعتصم عاصمة الدولة من بغداد إلى سامراء \n",
            "نقل(المعتصم عاصمة الدولة ,بغداد )\n",
            "نقل\n",
            "نقل(PER,LOC)\n",
            "Transfer(PER,LOC)\n",
            "S-PER B-GPE E-GPE \n",
            "S-LOC \n",
            "المعتصم عاصمة الدولة \n",
            "PER\n",
            "بغداد \n",
            "LOC\n",
            "-----------------------------\n",
            "The index of sentence is :  30\n",
            " الخُلفاء الفاطميّون  ضمّوا إلى مُمتلكاتهم جزيرة صقلية\n",
            "-----------------------------\n",
            "The index of sentence is :  31\n",
            "أسس الفاطميّون مدينة المهدية \n",
            "-----------------------------\n",
            "The index of sentence is :  32\n",
            "الفاطميّون أسسوا مدينة القاهرة \n",
            "أسسوا(مدينة القاهرة , )\n",
            "أسس\n",
            "أسس(LOC,LOC)\n",
            "establish(LOC,LOC)\n",
            "B-LOC E-LOC \n",
            "E-LOC \n",
            "مدينة القاهرة \n",
            "LOC\n",
            " \n",
            "LOC\n",
            "-----------------------------\n",
            "The index of sentence is :  33\n",
            "وفي عهد سلالة تشو هذه اتسعت الحدود غربًا وجنوبًا \n",
            "اتسعت(عهد ,تشو )\n",
            "اتسع\n",
            "اتسع(TIM,PER)\n",
            "wide(TIM,PER)\n",
            "S-TIM \n",
            "S-PER \n",
            "عهد \n",
            "TIM\n",
            "تشو \n",
            "PER\n",
            "-----------------------------\n",
            "The index of sentence is :  34\n",
            "حقق الصينيون في عهد سلالة تشو تقدمًا عظيمًا\n",
            "حقق(الصينيون ,عهد )\n",
            "حقق\n",
            "حقق(GPE,TIM)\n",
            "Achieve(GPE,TIM)\n",
            "S-GPE \n",
            "S-TIM \n",
            "الصينيون \n",
            "GPE\n",
            "عهد \n",
            "TIM\n",
            "تقدما(الصينيون ,تشو )\n",
            "تقدم\n",
            "تقدم(GPE,PER)\n",
            "Progress(GPE,PER)\n",
            "S-GPE \n",
            "S-PER \n",
            "الصينيون \n",
            "GPE\n",
            "تشو \n",
            "PER\n",
            "-----------------------------\n",
            "The index of sentence is :  35\n",
            "تأسَّست روسيا على يد السلافيِّين الشرقيِّين \n",
            "-----------------------------\n",
            "The index of sentence is :  36\n",
            "تغلَّب إيفان الأوّل على التتار في إمارة تفير\n",
            "تغلب(إيفان ,التتار )\n",
            "تغلب\n",
            "تغلب(PER,GPE)\n",
            "get over(PER,GPE)\n",
            "B-PER \n",
            "S-GPE \n",
            "إيفان \n",
            "PER\n",
            "التتار \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  37\n",
            " إيفان الرابع اعتلى العرش ليصبح بذلك أوّل قيصر لروسيا\n",
            "اعتلى(إيفان ,العرش )\n",
            "اعتلى\n",
            "اعتلى(PER,MISC)\n",
            "climb(PER,MISC)\n",
            "B-PER \n",
            "S-MISC \n",
            "إيفان \n",
            "PER\n",
            "العرش \n",
            "MISC\n",
            "-----------------------------\n",
            "The index of sentence is :  38\n",
            "وقد أسَّس ميخائيل الأسرة الرومانوفيّة الحاكمة\n",
            "-----------------------------\n",
            "The index of sentence is :  39\n",
            " بطرس الأكبر  بنى العاصمة في عام 1703م\n",
            "بنى(العاصمة ,عام 1703م )\n",
            "بنى\n",
            "بنى(GPE,TIM)\n",
            "build(GPE,TIM)\n",
            "S-GPE \n",
            "B-TIM E-TIM \n",
            "العاصمة \n",
            "GPE\n",
            "عام 1703م \n",
            "TIM\n",
            "-----------------------------\n",
            "The index of sentence is :  40\n",
            "بعثت حكومة إثيوبيا رسالة إلى السودان \n",
            "بعثت(حكومة إثيوبيا ,السودان )\n",
            "بعث\n",
            "بعث(GPE,GPE)\n",
            "sent(GPE,GPE)\n",
            "S-GPE S-GPE \n",
            "S-GPE \n",
            "حكومة إثيوبيا \n",
            "GPE\n",
            "السودان \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  41\n",
            " السودان لا يزال يسيطر على أراض إثيوبية\n",
            "يزال(السودان ,إثيوبية )\n",
            "أزال\n",
            "أزال(GPE,GPE)\n",
            "remove(GPE,GPE)\n",
            "S-GPE \n",
            "S-GPE \n",
            "السودان \n",
            "GPE\n",
            "إثيوبية \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  42\n",
            " رئيس الوزراء المصري أن بلاده تعمل على مساعدة الليبيين \n",
            "تعمل(رئيس الوزراء ,المصري )\n",
            "عمل\n",
            "عمل(PRO,GPE)\n",
            "work(PRO,GPE)\n",
            "B-PRO E-PRO \n",
            "S-GPE \n",
            "رئيس الوزراء \n",
            "PRO\n",
            "المصري \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  43\n",
            "أنقرة والقاهرة جددتا رغبتهما في اتخاذ خطوات إضافية \n",
            "-----------------------------\n",
            "The index of sentence is :  44\n",
            " وزير الخارجية الإماراتي عبدالله بن زايد آل نهيان أجرى اتصالا هاتفيا بنظيره الإيراني حسين أمير عبد اللهيان\n",
            "أجرى(وزير الخارجية ,الإماراتي عبدالله بن زايد آل نهيان )\n",
            "جرى\n",
            "جرى(PRO,PER)\n",
            "run(PRO,PER)\n",
            "B-PRO E-PRO \n",
            "S-GPE B-PER I-PER I-PER I-PER E-PER \n",
            "وزير الخارجية \n",
            "PRO\n",
            "الإماراتي عبدالله بن زايد آل نهيان \n",
            "PER\n",
            "هاتفيا(وزير الخارجية ,الإيراني حسين أمير عبد اللهيان )\n",
            "هاتف\n",
            "هاتف(PRO,PER)\n",
            "make a call(PRO,PER)\n",
            "B-PRO E-PRO \n",
            "S-GPE B-PER I-PER I-PER E-PER \n",
            "وزير الخارجية \n",
            "PRO\n",
            "الإيراني حسين أمير عبد اللهيان \n",
            "PER\n",
            "-----------------------------\n",
            "The index of sentence is :  45\n",
            "وأشار آل نهيان إلى العلاقات التي تجمع بين شعبي البلدين\n",
            "تجمع(آل نهيان ,البلدين )\n",
            "تجمع\n",
            "تجمع(PER,GPE)\n",
            "gathering(PER,GPE)\n",
            "B-PER E-PER \n",
            "S-GPE \n",
            "آل نهيان \n",
            "PER\n",
            "البلدين \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  46\n",
            "وأشار غوليايف إلى أن الجيش الروسي باشر كذلك بتوزيع مساعدات إنسانية \n",
            "باشر(غوليايف ,الجيش الروسي )\n",
            "باشر\n",
            "باشر(PER,ORG)\n",
            "start(PER,ORG)\n",
            "S-PER \n",
            "S-ORG S-GPE \n",
            "غوليايف \n",
            "PER\n",
            "الجيش الروسي \n",
            "ORG\n",
            "-----------------------------\n",
            "The index of sentence is :  47\n",
            "في إطلاق حوار مباشر بين الإسرائيليين والفلسطينيين \n",
            "-----------------------------\n",
            "The index of sentence is :  48\n",
            "لاستمرار عملية تطبيع العلاقات بين إسرائيل وجيرانها العرب\n",
            "-----------------------------\n",
            "The index of sentence is :  49\n",
            "وقال لافروف أثناء مؤتمر صحفي مشترك عقده في موسكو \n",
            "وقال(لافروف ,موسكو )\n",
            "قال\n",
            "قال(PER,GPE)\n",
            "say(PER,GPE)\n",
            "S-PER \n",
            "S-GPE \n",
            "لافروف \n",
            "PER\n",
            "موسكو \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  50\n",
            "  الأمير هاري اتهم العائلة الملكية بالعنصرية\n",
            "اتهم(الأمير هاري ,الملكية )\n",
            "اتهم\n",
            "اتهم(PER,GPE)\n",
            "accuse(PER,GPE)\n",
            "S-PRO S-PER \n",
            "E-GPE \n",
            "الأمير هاري \n",
            "PER\n",
            "الملكية \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  51\n",
            "وقال ميغان إن عضوا في العائلة الملكية أعرب عن مخاوفه إزاء لون بشر طفلهما\n",
            "وقال(ميغان ,العائلة الملكية )\n",
            "قال\n",
            "قال(PER,GPE)\n",
            "say(PER,GPE)\n",
            "S-PER \n",
            "B-GPE E-GPE \n",
            "ميغان \n",
            "PER\n",
            "العائلة الملكية \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  52\n",
            "اعلنت كوريا الجنوبية عن إدراج المدعي العام السابق للبلاد يون سيوك- يول رسميا للتحقيق معه في تهم متعددة\n",
            "رسميا(كوريا الجنوبية ,المدعي العام )\n",
            "رسم\n",
            "رسم(GPE,PRO)\n",
            "draw(GPE,PRO)\n",
            "S-GPE E-GPE \n",
            "B-PRO E-PRO \n",
            "كوريا الجنوبية \n",
            "GPE\n",
            "المدعي العام \n",
            "PRO\n",
            "-----------------------------\n",
            "The index of sentence is :  53\n",
            "الكرملين ينظر بأسف إلى تصريح رئيس أوكرانيا فلاديمير زيلينسكي\n",
            "ينظر(الكرملين ,رئيس أوكرانيا فلاديمير زيلينسكي )\n",
            "نظر\n",
            "نظر(GPE,PER)\n",
            "look at(GPE,PER)\n",
            "S-GPE \n",
            "S-PRO S-GPE B-PER E-PER \n",
            "الكرملين \n",
            "GPE\n",
            "رئيس أوكرانيا فلاديمير زيلينسكي \n",
            "PER\n",
            "-----------------------------\n",
            "The index of sentence is :  54\n",
            "الرئيس الأمريكي جو بايدن أبلغ رئيس الوزراء الإسرائيلي نفتالي بينيت \n",
            "أبلغ(الرئيس الأمريكي جو بايدن ,رئيس الوزراء )\n",
            "بلغ\n",
            "بلغ(PER,PRO)\n",
            "reach(PER,PRO)\n",
            "S-PRO S-GPE B-PER E-PER \n",
            "B-PRO E-PRO \n",
            "الرئيس الأمريكي جو بايدن \n",
            "PER\n",
            "رئيس الوزراء \n",
            "PRO\n",
            "-----------------------------\n",
            "The index of sentence is :  55\n",
            "بادين طرح موضوع إعادة فتح القنصلية \n",
            "طرح(بادين ,القنصلية )\n",
            "طرح\n",
            "طرح(PER,GPE)\n",
            "Subtract(PER,GPE)\n",
            "S-PER \n",
            "S-GPE \n",
            "بادين \n",
            "PER\n",
            "القنصلية \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  56\n",
            "وزير الخارجية أنتوني بلينكن ذكر ذلك علنا خلال زيارته إلى إسرائيل\n",
            "ذكر(وزير الخارجية ,أنتوني بلينكن )\n",
            "ذكر\n",
            "ذكر(PRO,PER)\n",
            "mention(PRO,PER)\n",
            "B-PRO E-PRO \n",
            "B-PER E-PER \n",
            "وزير الخارجية \n",
            "PRO\n",
            "أنتوني بلينكن \n",
            "PER\n",
            "-----------------------------\n",
            "The index of sentence is :  57\n",
            "وأشار الموقع الإسرائيلي إلى أن إعادة فتح القنصلية في القدس تمثل مشكلة \n",
            "فتح(الإسرائيلي ,القنصلية )\n",
            "فتح\n",
            "فتح(GPE,GPE)\n",
            " open(GPE,GPE)\n",
            "S-GPE \n",
            "S-GPE \n",
            "الإسرائيلي \n",
            "GPE\n",
            "القنصلية \n",
            "GPE\n",
            "تمثل(الإسرائيلي ,القدس )\n",
            "تمثل\n",
            "تمثل(GPE,LOC)\n",
            "represent(GPE,LOC)\n",
            "S-GPE \n",
            "S-LOC \n",
            "الإسرائيلي \n",
            "GPE\n",
            "القدس \n",
            "LOC\n",
            "-----------------------------\n",
            "The index of sentence is :  58\n",
            "التقى البابا فرانسيس رئيسة مجلس النواب الأمريكي نانسي بيلوسي \n",
            "التقى(البابا فرانسيس رئيسة مجلس النواب ,الأمريكي نانسي بيلوسي )\n",
            "التقى\n",
            "التقى(PER,PER)\n",
            "meet(PER,PER)\n",
            "S-PRO S-PER B-PRO I-PRO E-PRO \n",
            "S-GPE B-PER E-PER \n",
            "البابا فرانسيس رئيسة مجلس النواب \n",
            "PER\n",
            "الأمريكي نانسي بيلوسي \n",
            "PER\n",
            "-----------------------------\n",
            "The index of sentence is :  59\n",
            "الرئيس الأمريكي جو بايدن  سيزور روما لإجراء محادثات \n",
            "سيزور(الرئيس الأمريكي جو بايدن ,روما )\n",
            "زور\n",
            "زور(PER,LOC)\n",
            "fake(PER,LOC)\n",
            "S-PRO S-GPE B-PER E-PER \n",
            "S-LOC \n",
            "الرئيس الأمريكي جو بايدن \n",
            "PER\n",
            "روما \n",
            "LOC\n",
            "-----------------------------\n",
            "The index of sentence is :  60\n",
            "وصلت المستشارة الألمانية أنغيلا ميركل إلى إسرائيل في وقت متأخر من السبت\n",
            "-----------------------------\n",
            "The index of sentence is :  61\n",
            "وغردت السفيرة الألمانية في إسرائيل\n",
            "-----------------------------\n",
            "The index of sentence is :  62\n",
            "حكمت محكمة  أولد بيلي  على الشرطي اللندني السابق واين كوزينز بالسجن \n",
            "حكمت(محكمة  ,أولد بيلي )\n",
            "حكم\n",
            "حكم(ORG,LOC)\n",
            "rule(ORG,LOC)\n",
            "B-LOC E-ORG \n",
            "I-LOC E-LOC \n",
            "محكمة  \n",
            "ORG\n",
            "أولد بيلي \n",
            "LOC\n",
            "بالسجن(أولد بيلي ,الشرطي اللندني )\n",
            "سجن\n",
            "سجن(LOC,PRO)\n",
            "prison(LOC,PRO)\n",
            "I-LOC E-LOC \n",
            "S-PRO S-GPE \n",
            "أولد بيلي \n",
            "LOC\n",
            "الشرطي اللندني \n",
            "PRO\n",
            "-----------------------------\n",
            "The index of sentence is :  63\n",
            "شرطة لندن تجري مقابلة مع امرأة \n",
            "-----------------------------\n",
            "The index of sentence is :  64\n",
            " امرأة تتهم الأمير أندرو باغتصابها \n",
            "تتهم(امرأة ,الأمير أندرو )\n",
            "اتهم\n",
            "اتهم(PER,PER)\n",
            "accuse(PER,PER)\n",
            "S-PER \n",
            "S-PRO S-PER \n",
            "امرأة \n",
            "PER\n",
            "الأمير أندرو \n",
            "PER\n",
            "-----------------------------\n",
            "The index of sentence is :  65\n",
            "تقع مدينة دبي على الساحل الشرقي من شبه الجزيرة العربية\n",
            "-----------------------------\n",
            "The index of sentence is :  66\n",
            "يعد برج خليفة البناء الرئيسي في دبي \n",
            "يعد(برج خليفة ,دبي )\n",
            "أعاد\n",
            "أعاد(LOC,LOC)\n",
            "repeat(LOC,LOC)\n",
            "B-LOC E-LOC \n",
            "S-LOC \n",
            "برج خليفة \n",
            "LOC\n",
            "دبي \n",
            "LOC\n",
            "-----------------------------\n",
            "The index of sentence is :  67\n",
            " يقع متحف دبي في قلعة الفهيدي \n",
            "-----------------------------\n",
            "The index of sentence is :  68\n",
            "مغارة جعيتا عبارة عن كهوف من الحجر الجيري الكارستي \n",
            "-----------------------------\n",
            "The index of sentence is :  69\n",
            " سلطنة عُمان  تقع في الجهة الجنوبيّة الشرقيّة من شبه الجزيرة العربيّة\n",
            "-----------------------------\n",
            "The index of sentence is :  70\n",
            "كما تُطلّ سلطنة عُمان على بحر العرب\n",
            "تطل(سلطنة عُمان ,بحر العرب )\n",
            "طلى\n",
            "طلى(GPE,GEO)\n",
            "plating(GPE,GEO)\n",
            "B-GPE E-GPE \n",
            "B-GEO E-GEO \n",
            "سلطنة عُمان \n",
            "GPE\n",
            "بحر العرب \n",
            "GEO\n",
            "-----------------------------\n",
            "The index of sentence is :  71\n",
            " مُحافظة مسقط تُعدّ العاصمة الرسميّة للسلطنة\n",
            "الرسمية(مسقط ,العاصمة )\n",
            "رسم\n",
            "رسم(LOC,GPE)\n",
            "draw(LOC,GPE)\n",
            "S-LOC \n",
            "S-GPE \n",
            "مسقط \n",
            "LOC\n",
            "العاصمة \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  72\n",
            "واستقلّت عُمان عن الاحتلال البرتغاليّ عام 1650م\n",
            "-----------------------------\n",
            "The index of sentence is :  73\n",
            " مُحافظة مسقط  تُعتبرالعاصمة الوطنيّة لعُمان\n",
            "-----------------------------\n",
            "The index of sentence is :  74\n",
            "توسكانا اقليم يقع في غرب ايطاليا \n",
            "-----------------------------\n",
            "The index of sentence is :  75\n",
            "تقع مدينة اللاذقية في أقصى شمال سوريا \n",
            "-----------------------------\n",
            "The index of sentence is :  76\n",
            "بحيرة مشقيتاوهي عبارة عن بحيرة كبيرة وجميلة تقع إلى شمال شرق اللاذقية\n",
            "-----------------------------\n",
            "The index of sentence is :  77\n",
            " تتكون جزر المالديف من حوالي 1،190 جزيرة مرجانية\n",
            "-----------------------------\n",
            "The index of sentence is :  78\n",
            "مدينة نيويورك تعتبر مركزاً للمال\n",
            "تعتبر(مدينة نيويورك ,للمال )\n",
            "اعتبر\n",
            "اعتبر(LOC,MISC)\n",
            "Considered(LOC,MISC)\n",
            "B-LOC E-LOC \n",
            "S-MISC \n",
            "مدينة نيويورك \n",
            "LOC\n",
            "للمال \n",
            "MISC\n",
            "-----------------------------\n",
            "The index of sentence is :  79\n",
            "تعتبر مدينة نيويورك عاصمة الولايات المتحدة الامريكية الاقتصادية\n",
            "تعتبر(مدينة نيويورك ,عاصمة الولايات المتحدة الامريكية )\n",
            "اعتبر\n",
            "اعتبر(LOC,GPE)\n",
            "Considered(LOC,GPE)\n",
            "B-LOC E-LOC \n",
            "S-GPE B-GPE I-GPE E-GPE \n",
            "مدينة نيويورك \n",
            "LOC\n",
            "عاصمة الولايات المتحدة الامريكية \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  80\n",
            "وكانت نيويورك عاصمة الولايات المتحدة الأمريكية من العام 1785 وحتى 1790\n",
            "-----------------------------\n",
            "The index of sentence is :  81\n",
            "تمتد مدينة القاهرة على طول نهر النيل\n",
            "تمتد(مدينة القاهرة ,نهر النيل )\n",
            "امتد\n",
            "امتد(LOC,GEO)\n",
            "spread(LOC,GEO)\n",
            "B-LOC E-LOC \n",
            "B-GEO E-GEO \n",
            "مدينة القاهرة \n",
            "LOC\n",
            "نهر النيل \n",
            "GEO\n",
            "-----------------------------\n",
            "The index of sentence is :  82\n",
            "تقع مدينة أسوان على نهر النيل \n",
            "-----------------------------\n",
            "The index of sentence is :  83\n",
            "توجد في أسوان جزيرة إليفانتين \n",
            "-----------------------------\n",
            "The index of sentence is :  84\n",
            " ساحل البحر الأحمر يمتلئ بالكثير من أنواع الأسماك والشعاب المرجانية \n",
            "-----------------------------\n",
            "The index of sentence is :  85\n",
            "تقع كندا شمال القارة الأمريكية الشمالية\n",
            "-----------------------------\n",
            "The index of sentence is :  86\n",
            "وتقع العاصمة أوتاوا في مقاطعة أونتاريو\n",
            "-----------------------------\n",
            "The index of sentence is :  87\n",
            "تعرف العراق باسم بلاد ما بين النهرين\n",
            "تعرف(العراق ,بلاد ما بين النهرين )\n",
            "تعرف\n",
            "تعرف(GPE,LOC)\n",
            "know or introduce self(GPE,LOC)\n",
            "S-GPE \n",
            "B-GPE I-LOC I-LOC E-LOC \n",
            "العراق \n",
            "GPE\n",
            "بلاد ما بين النهرين \n",
            "LOC\n",
            "-----------------------------\n",
            "The index of sentence is :  88\n",
            "سُورِيَة دولة عربية تعد جمهورية مركزية\n",
            "-----------------------------\n",
            "The index of sentence is :  89\n",
            "سورية تقع ضمن منطقة الشرق الأوسط \n",
            "ضمن(سورية ,الشرق الأوسط )\n",
            "ضمن\n",
            "ضمن(GPE,GPE)\n",
            "guarantee(GPE,GPE)\n",
            "S-GPE \n",
            "B-GPE E-GPE \n",
            "سورية \n",
            "GPE\n",
            "الشرق الأوسط \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  90\n",
            "تطل سوريا على البحر الأبيض المتوسط\n",
            "تطل(سوريا ,البحر الأبيض المتوسط )\n",
            "طلى\n",
            "طلى(GPE,GEO)\n",
            "plating(GPE,GEO)\n",
            "S-GPE \n",
            "B-GEO I-GEO E-GEO \n",
            "سوريا \n",
            "GPE\n",
            "البحر الأبيض المتوسط \n",
            "GEO\n",
            "-----------------------------\n",
            "The index of sentence is :  91\n",
            "روسيا الاتحادية ‏ هي دولة تقع في شمال أوراسيا\n",
            "-----------------------------\n",
            "The index of sentence is :  92\n",
            " يقع نطاق التندرا في أقصى شمالي روسيا\n",
            "-----------------------------\n",
            "The index of sentence is :  93\n",
            "تقع بحيرة لادوغا قرب سانت بطرسبرغ \n",
            "قرب(بحيرة لادوغا ,سانت بطرسبرغ )\n",
            "قرب\n",
            "قرب(GEO,GEO)\n",
            "make it closer(GEO,GEO)\n",
            "B-GEO E-GEO \n",
            "B-GEO E-GEO \n",
            "بحيرة لادوغا \n",
            "GEO\n",
            "سانت بطرسبرغ \n",
            "GEO\n",
            "-----------------------------\n",
            "The index of sentence is :  94\n",
            "نهر الفولغا ينبع من تلال فالداي غربي موسكو\n",
            "-----------------------------\n",
            "The index of sentence is :  95\n",
            " مهاجم المنتخب السعودي صالح الشهر يسجيل هدف المباراة الوحيد\n",
            "-----------------------------\n",
            "The index of sentence is :  96\n",
            "ويعد المنتخب العراقي الخاسر الأكبر من بين المنتخبات العربية حتى الآن\n",
            "ويعد(المنتخب العراقي ,المنتخبات العربية )\n",
            "أعاد\n",
            "أعاد(ORG,ORG)\n",
            "repeat(ORG,ORG)\n",
            "S-ORG S-GPE \n",
            "S-ORG S-GPE \n",
            "المنتخب العراقي \n",
            "ORG\n",
            "المنتخبات العربية \n",
            "ORG\n",
            "-----------------------------\n",
            "The index of sentence is :  97\n",
            "علي مبخوت سجل هدف الإمارات \n",
            "سجل(علي مبخوت ,هدف الإمارات )\n",
            "سجل\n",
            "سجل(PER,MISC)\n",
            "register(PER,MISC)\n",
            "B-PER E-PER \n",
            "S-MISC S-GPE \n",
            "علي مبخوت \n",
            "PER\n",
            "هدف الإمارات \n",
            "MISC\n",
            "-----------------------------\n",
            "The index of sentence is :  98\n",
            "هدف سوريا حمل توقيع محمود البحر \n",
            "حمل(هدف سوريا ,محمود البحر )\n",
            "حمل\n",
            "حمل(MISC,PER)\n",
            "carry(MISC,PER)\n",
            "S-MISC S-GPE \n",
            "B-PER E-PER \n",
            "هدف سوريا \n",
            "MISC\n",
            "محمود البحر \n",
            "PER\n",
            "-----------------------------\n",
            "The index of sentence is :  99\n",
            " مدرب المنتخب السوري أخرج خريبين \n",
            "أخرج(مدرب المنتخب السوري ,خريبين )\n",
            "خرج\n",
            "خرج(PRO,PER)\n",
            "go out(PRO,PER)\n",
            "S-PRO S-ORG S-GPE \n",
            "S-PER \n",
            "مدرب المنتخب السوري \n",
            "PRO\n",
            "خريبين \n",
            "PER\n",
            "-----------------------------\n",
            "The index of sentence is :  100\n",
            "واستفاد مبخوت من خطأ قاتل للحارس السوري في الدقيقة 11\n",
            "واستفاد(مبخوت ,للحارس السوري )\n",
            "استفاد\n",
            "استفاد(PER,PRO)\n",
            "Benefit(PER,PRO)\n",
            "S-PER \n",
            "S-PRO S-GPE \n",
            "مبخوت \n",
            "PER\n",
            "للحارس السوري \n",
            "PRO\n",
            "-----------------------------\n",
            "The index of sentence is :  101\n",
            "وقال السباعي في تصريحات لكووورة\n",
            "وقال(السباعي ,لكووورة )\n",
            "قال\n",
            "قال(PER,ORG)\n",
            "say(PER,ORG)\n",
            "S-PER \n",
            "S-ORG \n",
            "السباعي \n",
            "PER\n",
            "لكووورة \n",
            "ORG\n",
            "-----------------------------\n",
            "The index of sentence is :  102\n",
            "أمام الإمارات كان أداء نسور قاسيون مقبولا\n",
            "-----------------------------\n",
            "The index of sentence is :  103\n",
            "قال المدرب السوري عماد خانكان اليوم الأربعاء\n",
            "قال(المدرب السوري عماد خانكان ,اليوم الأربعاء )\n",
            "قال\n",
            "قال(PER,TIM)\n",
            "say(PER,TIM)\n",
            "S-PRO S-GPE B-PER E-PER \n",
            "B-TIM E-TIM \n",
            "المدرب السوري عماد خانكان \n",
            "PER\n",
            "اليوم الأربعاء \n",
            "TIM\n",
            "-----------------------------\n",
            "The index of sentence is :  104\n",
            "المنتخب السوري لعب الشوط الأول بشكل مثالي\n",
            "-----------------------------\n",
            "The index of sentence is :  105\n",
            "أضاع منتخب سوريا فوزًا كان في المتناول أمام إيران\n",
            "أضاع(منتخب سوريا ,إيران )\n",
            "أضاع\n",
            "أضاع(ORG,GPE)\n",
            "lost(ORG,GPE)\n",
            "S-ORG S-GPE \n",
            "S-GPE \n",
            "منتخب سوريا \n",
            "ORG\n",
            "إيران \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  106\n",
            "قاد المدرب الهولندي فريقه الجديد للتعادل \n",
            "-----------------------------\n",
            "The index of sentence is :  107\n",
            "تحسن أداء العراق في الشوط الثاني \n",
            "تحسن(العراق ,الشوط الثاني )\n",
            "تحسن\n",
            "تحسن(GPE,TIM)\n",
            "to improve(GPE,TIM)\n",
            "S-GPE \n",
            "B-TIM E-TIM \n",
            "العراق \n",
            "GPE\n",
            "الشوط الثاني \n",
            "TIM\n",
            "-----------------------------\n",
            "The index of sentence is :  108\n",
            "ويفتتح منتخب العراق مشواره بمواجهة كوريا الجنوبية\n",
            "ويفتتح(منتخب العراق ,كوريا الجنوبية )\n",
            "افتتح\n",
            "افتتح(ORG,GPE)\n",
            "opened(ORG,GPE)\n",
            "S-ORG S-GPE \n",
            "B-GPE E-GPE \n",
            "منتخب العراق \n",
            "ORG\n",
            "كوريا الجنوبية \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  109\n",
            "ويحلم منتخب أسود الرافدين ببلوغ المونديال\n",
            "ويحلم(منتخب أسود الرافدين ,المونديال )\n",
            "حلم\n",
            "حلم(ORG,MISC)\n",
            "dream(ORG,MISC)\n",
            "B-ORG I-ORG E-ORG \n",
            "S-MISC \n",
            "منتخب أسود الرافدين \n",
            "ORG\n",
            "المونديال \n",
            "MISC\n",
            "-----------------------------\n",
            "The index of sentence is :  110\n",
            "مغادرة الفريق لخوض مباراتي كوريا الجنوبية وإيران\n",
            "-----------------------------\n",
            "The index of sentence is :  111\n",
            "المنتخب السوري سيصل الدوحة\n",
            "-----------------------------\n",
            "The index of sentence is :  112\n",
            "منتخب السعودية يفوز على أوزبكستان \n",
            "يفوز(منتخب السعودية ,أوزبكستان )\n",
            "فاز\n",
            "فاز(ORG,GPE)\n",
            "win(ORG,GPE)\n",
            "S-ORG S-GPE \n",
            "S-GPE \n",
            "منتخب السعودية \n",
            "ORG\n",
            "أوزبكستان \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  113\n",
            "الأمراض الروماتيزمية تؤثر في أكثر من 46 مليون أمريكي\n",
            "-----------------------------\n",
            "The index of sentence is :  114\n",
            "تتفاوت أعراض البروستاتا عند الرجال \n",
            "-----------------------------\n",
            "The index of sentence is :  115\n",
            "لا ترتبط حدّة الأعراض بحجم غدة البروستات\n",
            "-----------------------------\n",
            "The index of sentence is :  116\n",
            "يُعاني بعض المصابين بتضخمٍ بسيطٍ بغدة البروستات \n",
            "يعاني(المصابين ,بغدة البروستات )\n",
            "عانى\n",
            "عانى(PER,MISC)\n",
            "suffer(PER,MISC)\n",
            "S-PER \n",
            "B-MISC E-MISC \n",
            "المصابين \n",
            "PER\n",
            "بغدة البروستات \n",
            "MISC\n",
            "-----------------------------\n",
            "The index of sentence is :  117\n",
            " يحدث التهاب المهبل عادة نتيجة تغير التوازن الطبيعي للبكتيريا المهبلية\n",
            "يحدث(التهاب المهبل ,للبكتيريا المهبلية )\n",
            "حدث\n",
            "حدث(DIS,MISC)\n",
            "happened(DIS,MISC)\n",
            "B-DIS E-DIS \n",
            "B-MISC E-MISC \n",
            "التهاب المهبل \n",
            "DIS\n",
            "للبكتيريا المهبلية \n",
            "MISC\n",
            "-----------------------------\n",
            "The index of sentence is :  118\n",
            " يحدث داء المشعرات نتيجة العدوى بكائن طفيلي \n",
            "-----------------------------\n",
            "The index of sentence is :  119\n",
            "تتسبب مشكلة السكتات الدماغية في موت أو تلف خلايا الدماغ\n",
            "تتسبب(السكتات الدماغية ,خلايا الدماغ )\n",
            "تسبب\n",
            "تسبب(DIS,MISC)\n",
            "causes(DIS,MISC)\n",
            "B-DIS E-DIS \n",
            "B-MISC E-MISC \n",
            "السكتات الدماغية \n",
            "DIS\n",
            "خلايا الدماغ \n",
            "MISC\n",
            "-----------------------------\n",
            "The index of sentence is :  120\n",
            "يسبب مرض سرطان المريء صعوبةً في البلع\n",
            "يسبب(مرض سرطان المريء ,صعوبةً )\n",
            "سب\n",
            "سب(DIS,DIS)\n",
            "insult(DIS,DIS)\n",
            "B-DIS B-DIS E-DIS \n",
            "E-DIS \n",
            "مرض سرطان المريء \n",
            "DIS\n",
            "صعوبةً \n",
            "DIS\n",
            "-----------------------------\n",
            "The index of sentence is :  121\n",
            "مرضى الإيدز يعانون بشكلٍ كبيرٍ من التهاب المريء\n",
            "يعانون(مرضى الإيدز ,التهاب المريء )\n",
            "أعان\n",
            "أعان(DIS,DIS)\n",
            "help(DIS,DIS)\n",
            "B-DIS E-DIS \n",
            "B-DIS E-DIS \n",
            "مرضى الإيدز \n",
            "DIS\n",
            "التهاب المريء \n",
            "DIS\n",
            "-----------------------------\n",
            "The index of sentence is :  122\n",
            "يُعتبر الزكام أكثر الأمراض المُعدية التي تُصيب الإنسان\n",
            "يعتبر(الزكام ,الأمراض )\n",
            "اعتبر\n",
            "اعتبر(DIS,DIS)\n",
            "Considered(DIS,DIS)\n",
            "S-DIS \n",
            "B-DIS \n",
            "الزكام \n",
            "DIS\n",
            "الأمراض \n",
            "DIS\n",
            "-----------------------------\n",
            "The index of sentence is :  123\n",
            "الفيروسات التي تُهاجم الجهاز التنفسيّ العلويّ \n",
            "-----------------------------\n",
            "The index of sentence is :  124\n",
            "أكثر الفيروسات التي تتسبب بهذا المرض  فيروس كورونا \n",
            "تتسبب(الفيروسات ,المرض  )\n",
            "تسبب\n",
            "تسبب(MISC,DIS)\n",
            "causes(MISC,DIS)\n",
            "S-MISC \n",
            "S-DIS E-MISC \n",
            "الفيروسات \n",
            "MISC\n",
            "المرض  \n",
            "DIS\n",
            "-----------------------------\n",
            "The index of sentence is :  125\n",
            " النّاسور المعوي الجلدي  ويربط بين الأمعاء الدّقيقة والجلد\n",
            "ويربط(المعوي الجلدي ,الأمعاء )\n",
            "ربط\n",
            "ربط(DIS,MISC)\n",
            "link(DIS,MISC)\n",
            "S-MISC S-DIS \n",
            "S-MISC \n",
            "المعوي الجلدي \n",
            "DIS\n",
            "الأمعاء \n",
            "MISC\n",
            "-----------------------------\n",
            "The index of sentence is :  126\n",
            " تراكم الزوائد الأنفية عبارة عن نمو زائد لأنسجة الأنف\n",
            "تراكم(الزوائد الأنفية ,الأنف )\n",
            "تراكم\n",
            "تراكم(MISC,MISC)\n",
            "Accumulation(MISC,MISC)\n",
            "B-MISC E-MISC \n",
            "S-MISC \n",
            "الزوائد الأنفية \n",
            "MISC\n",
            "الأنف \n",
            "MISC\n",
            "-----------------------------\n",
            "The index of sentence is :  127\n",
            "يتم علاج التهاب اللوزتين بالمضادات الحيوية المناسبة\n",
            "-----------------------------\n",
            "The index of sentence is :  128\n",
            " أحد مضاعفات التهاب اللوزتين قد يُسبب انسداد مجرى التنفس\n",
            "-----------------------------\n",
            "The index of sentence is :  129\n",
            "النقرس يُعدّ أحد أنواع التهابات المفاصل\n",
            "يعد(النقرس ,التهابات المفاصل )\n",
            "أعاد\n",
            "أعاد(DIS,DIS)\n",
            "repeat(DIS,DIS)\n",
            "B-DIS \n",
            "B-DIS E-DIS \n",
            "النقرس \n",
            "DIS\n",
            "التهابات المفاصل \n",
            "DIS\n",
            "-----------------------------\n",
            "The index of sentence is :  130\n",
            " تطبيق يتيح للمستخدمين التواصل عن طريق حركات الوجه\n",
            "-----------------------------\n",
            "The index of sentence is :  131\n",
            "يستهدف التطبيق بشكل خاص الأشخاص المصابين بالتصلب اللويحي\n",
            "-----------------------------\n",
            "The index of sentence is :  132\n",
            "وبات تطبيق  بروجكت أكتيفايت  متوافراً بالإنجليزية\n",
            "وبات(تطبيق ,بروجكت أكتيفايت )\n",
            "بات\n",
            "بات(MISC,MISC)\n",
            " become(MISC,MISC)\n",
            "B-MISC \n",
            "I-MISC E-MISC \n",
            "تطبيق \n",
            "MISC\n",
            "بروجكت أكتيفايت \n",
            "MISC\n",
            "-----------------------------\n",
            "The index of sentence is :  133\n",
            "تطلق المجموعة الأميركية العملاقة خاصية  كاميرا سويتشز  المدمجة \n",
            "تطلق(الأميركية ,كاميرا سويتشز )\n",
            "طلق\n",
            "طلق(GPE,MISC)\n",
            "divorced(GPE,MISC)\n",
            "S-GPE \n",
            "B-MISC E-MISC \n",
            "الأميركية \n",
            "GPE\n",
            "كاميرا سويتشز \n",
            "MISC\n",
            "-----------------------------\n",
            "The index of sentence is :  134\n",
            "أطلقت شركة مايكروسوفت إصدارات جديدة من أجهزتها الحاسوبية الذكية\n",
            "أطلقت(شركة مايكروسوفت ,أجهزتها الحاسوبية الذكية )\n",
            "طلق\n",
            "طلق(ORG,MISC)\n",
            "divorced(ORG,MISC)\n",
            "B-ORG E-ORG \n",
            "B-MISC I-MISC E-MISC \n",
            "شركة مايكروسوفت \n",
            "ORG\n",
            "أجهزتها الحاسوبية الذكية \n",
            "MISC\n",
            "-----------------------------\n",
            "The index of sentence is :  135\n",
            "الثغرة التي اكتشفها الباحث بارك مينشان\n",
            "-----------------------------\n",
            "The index of sentence is :  136\n",
            "تسعى المفوضية الأوروبية إلى توحيد أجهزة شحن الهواتف المحمولة \n",
            "تسعى(المفوضية الأوروبية ,أجهزة شحن )\n",
            "سعى\n",
            "سعى(ORG,MISC)\n",
            "endeavor(ORG,MISC)\n",
            "B-ORG E-ORG \n",
            "B-MISC E-MISC \n",
            "المفوضية الأوروبية \n",
            "ORG\n",
            "أجهزة شحن \n",
            "MISC\n",
            "-----------------------------\n",
            "The index of sentence is :  137\n",
            "وزارة الدفاع الليتوانية تحذر من استخدام هواتف أي شركة صينية\n",
            "تحذر(وزارة الدفاع الليتوانية ,هواتف )\n",
            "حذر\n",
            "حذر(GPE,MISC)\n",
            "careful(GPE,MISC)\n",
            "B-GPE I-GPE E-GPE \n",
            "S-MISC \n",
            "وزارة الدفاع الليتوانية \n",
            "GPE\n",
            "هواتف \n",
            "MISC\n",
            "-----------------------------\n",
            "The index of sentence is :  138\n",
            "وشدد مارجيريس أبوكيفيسيوس نائب وزير الدفاع الليتواني على ضرورة تجنب شراء أي هواتف صينية\n",
            "وشدد(مارجيريس أبوكيفيسيوس ,نائب وزير الدفاع )\n",
            "شدد\n",
            "شدد(PER,PRO)\n",
            "focus on(PER,PRO)\n",
            "B-PER E-PER \n",
            "B-PRO I-PRO E-PRO \n",
            "مارجيريس أبوكيفيسيوس \n",
            "PER\n",
            "نائب وزير الدفاع \n",
            "PRO\n",
            "تجنب(نائب وزير الدفاع ,الليتواني )\n",
            "تجنب\n",
            "تجنب(PRO,GPE)\n",
            "Avoid(PRO,GPE)\n",
            "B-PRO I-PRO E-PRO \n",
            "S-GPE \n",
            "نائب وزير الدفاع \n",
            "PRO\n",
            "الليتواني \n",
            "GPE\n",
            "-----------------------------\n",
            "The index of sentence is :  139\n",
            "أعلنت شركة هواوي نهاية العام الماضي قيامها ببيع علامتها التجارية الفرعية هونر\n",
            "-----------------------------\n",
            "The index of sentence is :  140\n",
            "هونر من تطرح هاتف يعمل بنظام تشغيل أندرويد\n",
            "تطرح(هونر ,هاتف )\n",
            "طرح\n",
            "طرح(ORG,MISC)\n",
            "Subtract(ORG,MISC)\n",
            "S-ORG \n",
            "S-MISC \n",
            "هونر \n",
            "ORG\n",
            "هاتف \n",
            "MISC\n",
            "يعمل(هونر ,بنظام تشغيل أندرويد )\n",
            "عمل\n",
            "عمل(ORG,MISC)\n",
            "work(ORG,MISC)\n",
            "S-ORG \n",
            "B-MISC I-MISC E-MISC \n",
            "هونر \n",
            "ORG\n",
            "بنظام تشغيل أندرويد \n",
            "MISC\n",
            "-----------------------------\n",
            "The index of sentence is :  141\n",
            "تقدم مفوض حماية خصوصية البيانات في أيرلند بطلب إيضاح لشركة فيسبوك \n",
            "-----------------------------\n",
            "The index of sentence is :  142\n",
            "كشفت شاومي عن نموذج تجريبي لنظارتها الذكية\n",
            "كشفت(شاومي ,لنظارتها الذكية )\n",
            "كشف\n",
            "كشف(ORG,MISC)\n",
            "discover(ORG,MISC)\n",
            "S-ORG \n",
            "B-MISC E-MISC \n",
            "شاومي \n",
            "ORG\n",
            "لنظارتها الذكية \n",
            "MISC\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xlwt\n",
        "\n",
        "def output(list_sentences,list_first_entity,list_second_entity,list_first_entity_labled_parts,list_second_entity_labled_parts,list_type_First_entity,list_type_second_entity,list_relation,list_arabic_relational_sentence,list_labled_relational_sentence,list_translated_labled_relational_sentence):\n",
        "  workbook = xlwt.Workbook()\n",
        "  sheet = workbook.add_sheet(\"Relation_Arabic_language\")\n",
        "  sheet.write(0, 0, 'sentence')\n",
        "  sheet.write(0, 1, 'first_entity')\n",
        "  sheet.write(0, 2, 'second_entity')\n",
        "  sheet.write(0, 3, 'first_entity_labled_parts')\n",
        "  sheet.write(0, 4, 'second_entity_labled_parts')\n",
        "  sheet.write(0, 5, 'type_First_entity')\n",
        "  sheet.write(0, 6, 'type_second_entity')\n",
        "  sheet.write(0, 7, 'relation')\n",
        "  sheet.write(0, 8, 'arabic_relational_sentence')\n",
        "  sheet.write(0, 9, 'labled_relational_sentence')\n",
        "  sheet.write(0, 10, 'translated_labled_relational_sentence')\n",
        "  \n",
        "  row_counter=1\n",
        "  for i in range(0,len(list_sentences)):\n",
        "    sheet.write(row_counter,0,str(list_sentences[i]))\n",
        "    sheet.write(row_counter,1,str(list_first_entity[i]))\n",
        "    sheet.write(row_counter,2,str(list_second_entity[i]))\n",
        "    sheet.write(row_counter,3,str(list_first_entity_labled_parts[i]))\n",
        "    sheet.write(row_counter,4,str(list_second_entity_labled_parts[i]))\n",
        "    sheet.write(row_counter,5,str(list_type_First_entity[i]))\n",
        "    sheet.write(row_counter,6,str(list_type_second_entity[i]))\n",
        "    sheet.write(row_counter,7,str(list_relation[i]))\n",
        "    sheet.write(row_counter,8,str(list_arabic_relational_sentence[i]))\n",
        "    sheet.write(row_counter,9,str(list_labled_relational_sentence[i]))\n",
        "    sheet.write(row_counter,10,str(list_translated_labled_relational_sentence[i]))\n",
        "\n",
        "    row_counter+=1\n",
        "\n",
        "  workbook.save('Extraction_Relation_arabic_language.xls')\n",
        "\n",
        "output(list_sentences,list_first_entity,list_second_entity,list_first_entity_labled_parts,list_second_entity_labled_parts,list_type_First_entity,list_type_second_entity,list_relation,list_arabic_relational_sentence,list_labled_relational_sentence,list_translated_labled_relational_sentence)"
      ],
      "metadata": {
        "id": "GYtWzRIExGYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#helper function\n",
        "# with using flat function\n",
        "list_sentences=flat_list(list_sentences)\n",
        "list_first_entity=flat_list(list_first_entity)\n",
        "list_second_entity=flat_list(list_second_entity)\n",
        "list_first_entity_labled_parts=flat_list(list_first_entity_labled_parts)\n",
        "list_second_entity_labled_parts=flat_list(list_second_entity_labled_parts)\n",
        "list_type_First_entity=flat_list(list_type_First_entity)\n",
        "list_type_second_entity=flat_list(list_type_second_entity)\n",
        "list_relation=flat_list(list_relation)\n",
        "list_arabic_relational_sentence=flat_list(list_arabic_relational_sentence)\n",
        "list_labled_relational_sentence=flat_list(list_labled_relational_sentence)\n",
        "list_translated_labled_relational_sentence=flat_list(list_translated_labled_relational_sentence)\n"
      ],
      "metadata": {
        "id": "Q5tJ_Syx4lYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output(list_sentences,list_first_entity,list_second_entity,list_first_entity_labled_parts,list_second_entity_labled_parts,list_type_First_entity,list_type_second_entity,list_relation,list_arabic_relational_sentence,list_labled_relational_sentence,list_translated_labled_relational_sentence)"
      ],
      "metadata": {
        "id": "fPj5F8Fds4SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VnWYoKZrtH69"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FDMy3Qai3_Nj",
        "Fr7el10g4VAk",
        "icp2sO-_5R9x",
        "3PnDpmVM5bEE",
        "UhjRg2Uu5sgL",
        "peqHs9BA80eY",
        "mVZo4IVgriHz",
        "AqYQ2y8artKV",
        "Imot9s5VTd37"
      ],
      "authorship_tag": "ABX9TyPOhRpK65DKXgC81lpbstsU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}